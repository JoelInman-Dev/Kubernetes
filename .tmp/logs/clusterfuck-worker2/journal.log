Apr 14 12:18:37 clusterfuck-worker2 systemd-journald[88]: Journal started
Apr 14 12:18:37 clusterfuck-worker2 systemd-journald[88]: Runtime Journal (/run/log/journal/d7aadf4e6f254aae9095d95e7410a594) is 8.0M, max 800.5M, 792.5M free.
Apr 14 12:18:37 clusterfuck-worker2 systemd-journald[88]: Runtime Journal (/run/log/journal/d7aadf4e6f254aae9095d95e7410a594) is 8.0M, max 800.5M, 792.5M free.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Starting Flush Journal to Persistent Storage...
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Finished Create Static Device Nodes in /dev.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Reached target Preparation for Local File Systems.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Reached target Local File Systems.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Condition check resulted in Set Up Additional Binary Formats being skipped.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Condition check resulted in Store a System Token in an EFI Variable being skipped.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Condition check resulted in Commit a transient machine-id on disk being skipped.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Condition check resulted in Rule-based Manager for Device Events and Files being skipped.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Reached target System Initialization.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Started Daily Cleanup of Temporary Directories.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Reached target Basic System.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Reached target Timer Units.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Starting containerd container runtime...
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Condition check resulted in kubelet: The Kubernetes Node Agent being skipped.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Started containerd container runtime.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Reached target Multi-User System.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Reached target Graphical Interface.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Starting Record Runlevel Change in UTMP...
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: systemd-update-utmp-runlevel.service: Deactivated successfully.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Finished Record Runlevel Change in UTMP.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Finished Flush Journal to Persistent Storage.
Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Startup finished in 1.526s.
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.669039382Z" level=info msg="starting containerd" revision=941215f4987b5d303cedbbf9f8c9c4fa99b903ff version=v1.6.19-46-g941215f49
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.866322100Z" level=info msg="loading plugin \"io.containerd.content.v1.content\"..." type=io.containerd.content.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.867600892Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.aufs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.898651014Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.aufs\"..." error="aufs is not supported (modprobe aufs failed: exit status 1 \"modprobe: FATAL: Module aufs not found in directory /lib/modules/5.15.49-linuxkit\\n\"): skip plugin" type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.898795978Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.899114857Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.btrfs (ext4) must be a btrfs filesystem to be used with the btrfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.899170270Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.devmapper\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.899187742Z" level=warning msg="failed to load plugin io.containerd.snapshotter.v1.devmapper" error="devmapper not configured"
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.899197268Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.native\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.899223026Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.overlayfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.899548541Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.zfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.900466664Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.zfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.900904350Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.fuse-overlayfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.902655174Z" level=info msg="loading plugin \"io.containerd.metadata.v1.bolt\"..." type=io.containerd.metadata.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.902703179Z" level=warning msg="could not use snapshotter devmapper in metadata plugin" error="devmapper not configured"
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.905473245Z" level=info msg="metadata content store policy set" policy=shared
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.907549622Z" level=info msg="loading plugin \"io.containerd.differ.v1.walking\"..." type=io.containerd.differ.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.907757189Z" level=info msg="loading plugin \"io.containerd.event.v1.exchange\"..." type=io.containerd.event.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.907776083Z" level=info msg="loading plugin \"io.containerd.gc.v1.scheduler\"..." type=io.containerd.gc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.908046824Z" level=info msg="loading plugin \"io.containerd.service.v1.introspection-service\"..." type=io.containerd.service.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.908133419Z" level=info msg="loading plugin \"io.containerd.service.v1.containers-service\"..." type=io.containerd.service.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.908200622Z" level=info msg="loading plugin \"io.containerd.service.v1.content-service\"..." type=io.containerd.service.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.908317069Z" level=info msg="loading plugin \"io.containerd.service.v1.diff-service\"..." type=io.containerd.service.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.908345862Z" level=info msg="loading plugin \"io.containerd.service.v1.images-service\"..." type=io.containerd.service.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.908439113Z" level=info msg="loading plugin \"io.containerd.service.v1.leases-service\"..." type=io.containerd.service.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.908468198Z" level=info msg="loading plugin \"io.containerd.service.v1.namespaces-service\"..." type=io.containerd.service.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.908482557Z" level=info msg="loading plugin \"io.containerd.service.v1.snapshots-service\"..." type=io.containerd.service.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.908495578Z" level=info msg="loading plugin \"io.containerd.runtime.v1.linux\"..." type=io.containerd.runtime.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.909176451Z" level=info msg="loading plugin \"io.containerd.runtime.v2.task\"..." type=io.containerd.runtime.v2
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.910148678Z" level=info msg="loading plugin \"io.containerd.monitor.v1.cgroups\"..." type=io.containerd.monitor.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.910675962Z" level=info msg="loading plugin \"io.containerd.service.v1.tasks-service\"..." type=io.containerd.service.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.911351296Z" level=info msg="loading plugin \"io.containerd.grpc.v1.introspection\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.911382296Z" level=info msg="loading plugin \"io.containerd.internal.v1.restart\"..." type=io.containerd.internal.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.911615068Z" level=info msg="loading plugin \"io.containerd.grpc.v1.containers\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.911641581Z" level=info msg="loading plugin \"io.containerd.grpc.v1.content\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.911656408Z" level=info msg="loading plugin \"io.containerd.grpc.v1.diff\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.911667449Z" level=info msg="loading plugin \"io.containerd.grpc.v1.events\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912101592Z" level=info msg="loading plugin \"io.containerd.grpc.v1.healthcheck\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912127834Z" level=info msg="loading plugin \"io.containerd.grpc.v1.images\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912145555Z" level=info msg="loading plugin \"io.containerd.grpc.v1.leases\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912157990Z" level=info msg="loading plugin \"io.containerd.grpc.v1.namespaces\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912172716Z" level=info msg="loading plugin \"io.containerd.internal.v1.opt\"..." type=io.containerd.internal.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912312412Z" level=info msg="loading plugin \"io.containerd.grpc.v1.snapshots\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912383018Z" level=info msg="loading plugin \"io.containerd.grpc.v1.tasks\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912399187Z" level=info msg="loading plugin \"io.containerd.grpc.v1.version\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912513238Z" level=info msg="loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." type=io.containerd.tracing.processor.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912537924Z" level=info msg="skip loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." error="no OpenTelemetry endpoint: skip plugin" type=io.containerd.tracing.processor.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912552458Z" level=info msg="loading plugin \"io.containerd.internal.v1.tracing\"..." type=io.containerd.internal.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.912569387Z" level=error msg="failed to initialize a tracing processor \"otlp\"" error="no OpenTelemetry endpoint: skip plugin"
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.913341039Z" level=info msg="loading plugin \"io.containerd.grpc.v1.cri\"..." type=io.containerd.grpc.v1
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.922793019Z" level=info msg="Start cri plugin with config {PluginConfig:{ContainerdConfig:{Snapshotter:overlayfs DefaultRuntimeName:runc DefaultRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} UntrustedWorkloadRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} Runtimes:map[runc:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[SystemdCgroup:true] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:/etc/containerd/cri-base.json NetworkPluginConfDir: NetworkPluginMaxConfNum:0} test-handler:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[SystemdCgroup:true] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:/etc/containerd/cri-base.json NetworkPluginConfDir: NetworkPluginMaxConfNum:0}] NoPivot:false DisableSnapshotAnnotations:true DiscardUnpackedLayers:true IgnoreRdtNotEnabledErrors:false} CniConfig:{NetworkPluginBinDir:/opt/cni/bin NetworkPluginConfDir:/etc/cni/net.d NetworkPluginMaxConfNum:1 NetworkPluginConfTemplate: IPPreference:} Registry:{ConfigPath: Mirrors:map[] Configs:map[] Auths:map[] Headers:map[]} ImageDecryption:{KeyModel:node} DisableTCPService:true StreamServerAddress:127.0.0.1 StreamServerPort:0 StreamIdleTimeout:4h0m0s EnableSelinux:false SelinuxCategoryRange:1024 SandboxImage:registry.k8s.io/pause:3.7 StatsCollectPeriod:10 SystemdCgroup:false EnableTLSStreaming:false X509KeyPairStreaming:{TLSCertFile: TLSKeyFile:} MaxContainerLogLineSize:16384 DisableCgroup:false DisableApparmor:false RestrictOOMScoreAdj:false MaxConcurrentDownloads:3 DisableProcMount:false UnsetSeccompProfile: TolerateMissingHugetlbController:true DisableHugetlbController:true DeviceOwnershipFromSecurityContext:false IgnoreImageDefinedVolumes:false NetNSMountsUnderStateDir:false EnableUnprivilegedPorts:false EnableUnprivilegedICMP:false} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/run/containerd/io.containerd.grpc.v1.cri}"
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.923307563Z" level=info msg="Connect containerd service"
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.923532519Z" level=info msg="Get image filesystem path \"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\""
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.926382885Z" level=error msg="failed to load cni during init, please check CRI plugin status before setting up network for pods" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.958891309Z" level=info msg="Start subscribing containerd event"
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.958949961Z" level=info msg="Start recovering state"
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.972744454Z" level=warning msg="The image docker.io/kindest/kindnetd:v20230330-48f316cd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af is not unpacked."
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.975411272Z" level=warning msg="The image docker.io/kindest/local-path-helper:v20230330-48f316cd@sha256:135203f2441f916fb13dad1561d27f60a6f11f50ec288b01a7d2ee9947c36270 is not unpacked."
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.991313856Z" level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.991407381Z" level=info msg=serving... address=/run/containerd/containerd.sock
Apr 14 12:18:37 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:37.991430888Z" level=info msg="containerd successfully booted in 0.347655s"
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.005799723Z" level=warning msg="The image docker.io/kindest/local-path-provisioner:v0.0.23-kind.0@sha256:f2d0a02831ff3a03cf51343226670d5060623b43a4cfc4808bd0875b2c4b9501 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.031018339Z" level=warning msg="The image import-2023-03-30@sha256:3dd2337f70af979c7362b5e52bbdfcb3a5fd39c78d94d02145150cd2db86ba39 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.037511594Z" level=warning msg="The image import-2023-03-30@sha256:44db4d50a5f9c8efbac0d37ea974d1c0419a5928f90748d3d491a041a00c20b5 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.046051032Z" level=warning msg="The image import-2023-03-30@sha256:8dbb345de79d1c44f59a7895da702a5f71997ae72aea056609445c397b0c10dc is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.052619418Z" level=warning msg="The image import-2023-03-30@sha256:ba097b515c8c40689733c0f19de377e9bf8995964b7d7150c2045f3dfd166657 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.059898064Z" level=warning msg="The image registry.k8s.io/coredns/coredns:v1.9.3 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.089121489Z" level=warning msg="The image registry.k8s.io/etcd:3.5.6-0 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.121270690Z" level=warning msg="The image registry.k8s.io/kube-apiserver:v1.26.3 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.145201827Z" level=warning msg="The image registry.k8s.io/kube-controller-manager:v1.26.3 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.192035714Z" level=warning msg="The image registry.k8s.io/kube-proxy:v1.26.3 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.333220780Z" level=warning msg="The image registry.k8s.io/kube-scheduler:v1.26.3 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.335042859Z" level=warning msg="The image registry.k8s.io/pause:3.7 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.339175628Z" level=warning msg="The image sha256:221177c6082a88ea4f6240ab2450d540955ac6f4d5454f0e15751b653ebda165 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.354423200Z" level=warning msg="The image sha256:37af659db0ba1408025ceab0e9344dc354a8f38dd9b39875e48b2ad8e2fc3a51 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.377140490Z" level=warning msg="The image sha256:5185b96f0becf59032b8e3646e99f84d9655dff3ac9e2605e0dc77f9c441ae4a is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.408048864Z" level=warning msg="The image sha256:801fc1f38fa6cb63d4f64438ed884330e86e4be7504f33702e8edcd6fd2118a8 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.421378692Z" level=warning msg="The image sha256:a329ae3c2c52fe00e9c4eaf48b081cd184ee4bf9aea059e497f4965f0a8deedb is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.438564057Z" level=warning msg="The image sha256:c408b2276bb76627a6f633bf0d26052c208ebd51681c6c89866cc9647471c0bc is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.474776570Z" level=warning msg="The image sha256:cb77c367deebf699996aef822193e3630ce0fe9202b04f809214ff531718fe47 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.499295849Z" level=warning msg="The image sha256:dec886d0664924af97d276417712f4a065af16e8696ea66afc5a09799e137f8e is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.524152068Z" level=warning msg="The image sha256:eb3079d47a23af78d9b29f246a472fd77838412611e8ccb980a911b34663cc12 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.553235762Z" level=warning msg="The image sha256:fce326961ae2d51a5f726883fd59d2a8c2ccc3e45d3bb859882db58e422e59e7 is not unpacked."
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.578254018Z" level=info msg="Start event monitor"
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.578582267Z" level=info msg="Start snapshots syncer"
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.578744856Z" level=info msg="Start cni network conf syncer for default"
Apr 14 12:18:38 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:18:38.578800782Z" level=info msg="Start streaming server"
Apr 14 12:19:20 clusterfuck-worker2 systemd[1]: Reloading.
Apr 14 12:19:20 clusterfuck-worker2 systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Apr 14 12:19:20 clusterfuck-worker2 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: I0414 12:19:20.793840     170 server.go:198] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.145625     170 server.go:412] "Kubelet version" kubeletVersion="v1.26.3"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.145681     170 server.go:414] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.146392     170 server.go:836] "Client rotation is on, will bootstrap in background"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.152156     170 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153548     170 container_manager_linux.go:267] "Container manager verified user specified cgroup-root exists" cgroupRoot=[kubelet]
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153648     170 container_manager_linux.go:272] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/kubelet CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153819     170 topology_manager.go:134] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153875     170 container_manager_linux.go:308] "Creating device plugin manager"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153995     170 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.154239     170 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.163560     170 kubelet.go:398] "Attempting to sync node with API server"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.163621     170 kubelet.go:286] "Adding static pod path" path="/etc/kubernetes/manifests"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.163646     170 kubelet.go:297] "Adding apiserver pod source"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.163657     170 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.165522     170 kuberuntime_manager.go:244] "Container runtime initialized" containerRuntime="containerd" version="v1.6.19-46-g941215f49" apiVersion="v1"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.165783     170 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.166181     170 server.go:1186] "Started kubelet"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.167770     170 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.168248     170 server.go:161] "Starting to listen" address="0.0.0.0" port=10250
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.168975     170 server.go:451] "Adding debug handlers to kubelet server"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.182040     170 volume_manager.go:293] "Starting Kubelet Volume Manager"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.182580     170 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.204968     170 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205146     170 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.205301     170 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker2" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205358     170 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker2" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.205419     170 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205470     170 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205578     170 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker2.1755cc6329b2e204", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker2", UID:"clusterfuck-worker2", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker2"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 166166020, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 166166020, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205988     170 controller.go:146] failed to ensure lease exists, will retry in 200ms, error: leases.coordination.k8s.io "clusterfuck-worker2" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.254328     170 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker2.1755cc632ed2a885", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker2", UID:"clusterfuck-worker2", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker2 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker2"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252134533, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252134533, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.255818     170 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker2.1755cc632ed2be6b", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker2", UID:"clusterfuck-worker2", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker2 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker2"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252140139, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252140139, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.256516     170 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker2.1755cc632ed2c763", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker2", UID:"clusterfuck-worker2", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker2 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker2"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252142435, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252142435, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.258020     170 cpu_manager.go:214] "Starting CPU manager" policy="none"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.258067     170 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.258086     170 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.260230     170 policy_none.go:49] "None policy: Start"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.261297     170 memory_manager.go:169] "Starting memorymanager" policy="None"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.261351     170 state_mem.go:35] "Initializing new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.284447     170 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker2"
Apr 14 12:19:22 clusterfuck-worker2 systemd[1]: Created slice libcontainer container kubelet-kubepods.slice.
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.297847     170 kubelet.go:1466] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: root container [kubelet kubepods] doesn't exist"
Apr 14 12:19:22 clusterfuck-worker2 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Apr 14 12:19:22 clusterfuck-worker2 systemd[1]: kubelet.service: Failed with result 'exit-code'.
Apr 14 12:19:22 clusterfuck-worker2 systemd[1]: kubelet.service: Consumed 1.090s CPU time.
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1.
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: kubelet.service: Consumed 1.090s CPU time.
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.652468     217 server.go:198] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.662105     217 server.go:412] "Kubelet version" kubeletVersion="v1.26.3"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.662212     217 server.go:414] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.662492     217 server.go:836] "Client rotation is on, will bootstrap in background"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.663982     217 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.670853     217 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: W0414 12:19:23.671271     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676008     217 container_manager_linux.go:267] "Container manager verified user specified cgroup-root exists" cgroupRoot=[kubelet]
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676094     217 container_manager_linux.go:272] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/kubelet CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676119     217 topology_manager.go:134] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676133     217 container_manager_linux.go:308] "Creating device plugin manager"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676174     217 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.690636     217 kubelet.go:398] "Attempting to sync node with API server"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.690690     217 kubelet.go:286] "Adding static pod path" path="/etc/kubernetes/manifests"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.690759     217 kubelet.go:297] "Adding apiserver pod source"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.690773     217 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.692472     217 kuberuntime_manager.go:244] "Container runtime initialized" containerRuntime="containerd" version="v1.6.19-46-g941215f49" apiVersion="v1"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.693509     217 server.go:1186] "Started kubelet"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.699319     217 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.704230     217 server.go:161] "Starting to listen" address="0.0.0.0" port=10250
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.705047     217 server.go:451] "Adding debug handlers to kubelet server"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.716422     217 volume_manager.go:293] "Starting Kubelet Volume Manager"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.722176     217 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.747887     217 cpu_manager.go:214] "Starting CPU manager" policy="none"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.747909     217 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.747926     217 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.748073     217 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.748087     217 state_mem.go:96] "Updated CPUSet assignments" assignments=map[]
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.748093     217 policy_none.go:49] "None policy: Start"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.749018     217 memory_manager.go:169] "Starting memorymanager" policy="None"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.749038     217 state_mem.go:35] "Initializing new in-memory state store"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.749141     217 state_mem.go:75] "Updated machine memory state"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: E0414 12:19:23.753961     217 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"clusterfuck-worker2\" not found" node="clusterfuck-worker2"
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: Created slice libcontainer container kubelet-kubepods-besteffort.slice.
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: Created slice libcontainer container kubelet-kubepods-burstable.slice.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.789070     217 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.789428     217 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: E0414 12:19:23.794495     217 eviction_manager.go:261] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"clusterfuck-worker2\" not found"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.813979     217 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.825048     217 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker2"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.852018     217 kubelet_node_status.go:73] "Successfully registered node" node="clusterfuck-worker2"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.883200     217 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.883366     217 status_manager.go:176] "Starting to sync pod status with apiserver"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.883987     217 kubelet.go:2113] "Starting kubelet main sync loop"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: E0414 12:19:23.885137     217 kubelet.go:2137] "Skipping pod synchronization" err="PLEG is not healthy: pleg has yet to be successful"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.003569     217 kuberuntime_manager.go:1114] "Updating runtime config through cri with podcidr" CIDR="10.244.1.0/24"
Apr 14 12:19:24 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:24.004178558Z" level=info msg="No cni config template is specified, wait for other system components to drop the config."
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.004559     217 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.1.0/24"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.692514     217 apiserver.go:52] "Watching apiserver"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.694520     217 topology_manager.go:210] "Topology Admit Handler"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.694722     217 topology_manager.go:210] "Topology Admit Handler"
Apr 14 12:19:24 clusterfuck-worker2 systemd[1]: Created slice libcontainer container kubelet-kubepods-pode7826347_ae03_4685_ba88_c51ebe1c48d9.slice.
Apr 14 12:19:24 clusterfuck-worker2 systemd[1]: Created slice libcontainer container kubelet-kubepods-besteffort-podce844715_cb8a_44d7_8bfd_75f7392433d9.slice.
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.723376     217 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.728850     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/e7826347-ae03-4685-ba88-c51ebe1c48d9-xtables-lock\") pod \"kindnet-vxxpt\" (UID: \"e7826347-ae03-4685-ba88-c51ebe1c48d9\") " pod="kube-system/kindnet-vxxpt"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.728990     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/e7826347-ae03-4685-ba88-c51ebe1c48d9-lib-modules\") pod \"kindnet-vxxpt\" (UID: \"e7826347-ae03-4685-ba88-c51ebe1c48d9\") " pod="kube-system/kindnet-vxxpt"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729135     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zz9h4\" (UniqueName: \"kubernetes.io/projected/e7826347-ae03-4685-ba88-c51ebe1c48d9-kube-api-access-zz9h4\") pod \"kindnet-vxxpt\" (UID: \"e7826347-ae03-4685-ba88-c51ebe1c48d9\") " pod="kube-system/kindnet-vxxpt"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729241     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/ce844715-cb8a-44d7-8bfd-75f7392433d9-kube-proxy\") pod \"kube-proxy-rtmjr\" (UID: \"ce844715-cb8a-44d7-8bfd-75f7392433d9\") " pod="kube-system/kube-proxy-rtmjr"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729379     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/ce844715-cb8a-44d7-8bfd-75f7392433d9-xtables-lock\") pod \"kube-proxy-rtmjr\" (UID: \"ce844715-cb8a-44d7-8bfd-75f7392433d9\") " pod="kube-system/kube-proxy-rtmjr"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729524     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ce844715-cb8a-44d7-8bfd-75f7392433d9-lib-modules\") pod \"kube-proxy-rtmjr\" (UID: \"ce844715-cb8a-44d7-8bfd-75f7392433d9\") " pod="kube-system/kube-proxy-rtmjr"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729672     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sc456\" (UniqueName: \"kubernetes.io/projected/ce844715-cb8a-44d7-8bfd-75f7392433d9-kube-api-access-sc456\") pod \"kube-proxy-rtmjr\" (UID: \"ce844715-cb8a-44d7-8bfd-75f7392433d9\") " pod="kube-system/kube-proxy-rtmjr"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729854     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/e7826347-ae03-4685-ba88-c51ebe1c48d9-cni-cfg\") pod \"kindnet-vxxpt\" (UID: \"e7826347-ae03-4685-ba88-c51ebe1c48d9\") " pod="kube-system/kindnet-vxxpt"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729974     217 reconciler.go:41] "Reconciler: start to sync state"
Apr 14 12:19:25 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:25.613628241Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-rtmjr,Uid:ce844715-cb8a-44d7-8bfd-75f7392433d9,Namespace:kube-system,Attempt:0,}"
Apr 14 12:19:25 clusterfuck-worker2 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount2423268083.mount: Deactivated successfully.
Apr 14 12:19:25 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:25.675861066Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Apr 14 12:19:25 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:25.675919890Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Apr 14 12:19:25 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:25.675930353Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Apr 14 12:19:25 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:25.678163218Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/581aaab1d2ab4eff99246e0c8effa5a3e59c1d40c6f23a551abb232a3fc1ccb7 pid=285 runtime=io.containerd.runc.v2
Apr 14 12:19:25 clusterfuck-worker2 systemd[1]: Started libcontainer container 581aaab1d2ab4eff99246e0c8effa5a3e59c1d40c6f23a551abb232a3fc1ccb7.
Apr 14 12:19:25 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:25.777361167Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-rtmjr,Uid:ce844715-cb8a-44d7-8bfd-75f7392433d9,Namespace:kube-system,Attempt:0,} returns sandbox id \"581aaab1d2ab4eff99246e0c8effa5a3e59c1d40c6f23a551abb232a3fc1ccb7\""
Apr 14 12:19:25 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:25.786504442Z" level=info msg="CreateContainer within sandbox \"581aaab1d2ab4eff99246e0c8effa5a3e59c1d40c6f23a551abb232a3fc1ccb7\" for container &ContainerMetadata{Name:kube-proxy,Attempt:0,}"
Apr 14 12:19:25 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:25.905437964Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-vxxpt,Uid:e7826347-ae03-4685-ba88-c51ebe1c48d9,Namespace:kube-system,Attempt:0,}"
Apr 14 12:19:26 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:26.604642736Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Apr 14 12:19:26 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:26.604761952Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Apr 14 12:19:26 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:26.604788520Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Apr 14 12:19:26 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:26.605008223Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/7272e084d134b3f841bd442496f919cd0f3dd4e4854b96aa86768b2da2904f35 pid=326 runtime=io.containerd.runc.v2
Apr 14 12:19:26 clusterfuck-worker2 systemd[1]: Started libcontainer container 7272e084d134b3f841bd442496f919cd0f3dd4e4854b96aa86768b2da2904f35.
Apr 14 12:19:26 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:26.982603391Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-vxxpt,Uid:e7826347-ae03-4685-ba88-c51ebe1c48d9,Namespace:kube-system,Attempt:0,} returns sandbox id \"7272e084d134b3f841bd442496f919cd0f3dd4e4854b96aa86768b2da2904f35\""
Apr 14 12:19:26 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:26.992451930Z" level=info msg="PullImage \"docker.io/kindest/kindnetd:v20230330-48f316cd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af\""
Apr 14 12:19:27 clusterfuck-worker2 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount3770535228.mount: Deactivated successfully.
Apr 14 12:19:27 clusterfuck-worker2 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount861070398.mount: Deactivated successfully.
Apr 14 12:19:27 clusterfuck-worker2 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount3814784377.mount: Deactivated successfully.
Apr 14 12:19:27 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:27.555805407Z" level=info msg="CreateContainer within sandbox \"581aaab1d2ab4eff99246e0c8effa5a3e59c1d40c6f23a551abb232a3fc1ccb7\" for &ContainerMetadata{Name:kube-proxy,Attempt:0,} returns container id \"90ea38e66d02b2059dd3e2365a465b77f00f8527d81abda233090ef39216c7fd\""
Apr 14 12:19:27 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:27.557045509Z" level=info msg="StartContainer for \"90ea38e66d02b2059dd3e2365a465b77f00f8527d81abda233090ef39216c7fd\""
Apr 14 12:19:27 clusterfuck-worker2 systemd[1]: Started libcontainer container 90ea38e66d02b2059dd3e2365a465b77f00f8527d81abda233090ef39216c7fd.
Apr 14 12:19:27 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:27.668163827Z" level=info msg="StartContainer for \"90ea38e66d02b2059dd3e2365a465b77f00f8527d81abda233090ef39216c7fd\" returns successfully"
Apr 14 12:19:27 clusterfuck-worker2 kubelet[217]: I0414 12:19:27.928385     217 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-rtmjr" podStartSLOduration=4.928342753 pod.CreationTimestamp="2023-04-14 12:19:23 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-04-14 12:19:27.928214066 +0000 UTC m=+4.347222233" watchObservedRunningTime="2023-04-14 12:19:27.928342753 +0000 UTC m=+4.347350924"
Apr 14 12:19:28 clusterfuck-worker2 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount3704552971.mount: Deactivated successfully.
Apr 14 12:19:29 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:29.242086310Z" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/kindest/kindnetd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 12:19:29 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:29.244395149Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:a329ae3c2c52fe00e9c4eaf48b081cd184ee4bf9aea059e497f4965f0a8deedb,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 12:19:29 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:29.246156945Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/kindest/kindnetd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 12:19:29 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:29.246671589Z" level=info msg="PullImage \"docker.io/kindest/kindnetd:v20230330-48f316cd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af\" returns image reference \"sha256:a329ae3c2c52fe00e9c4eaf48b081cd184ee4bf9aea059e497f4965f0a8deedb\""
Apr 14 12:19:29 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:29.252736012Z" level=info msg="CreateContainer within sandbox \"7272e084d134b3f841bd442496f919cd0f3dd4e4854b96aa86768b2da2904f35\" for container &ContainerMetadata{Name:kindnet-cni,Attempt:0,}"
Apr 14 12:19:29 clusterfuck-worker2 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount1619669453.mount: Deactivated successfully.
Apr 14 12:19:29 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:29.268833043Z" level=info msg="CreateContainer within sandbox \"7272e084d134b3f841bd442496f919cd0f3dd4e4854b96aa86768b2da2904f35\" for &ContainerMetadata{Name:kindnet-cni,Attempt:0,} returns container id \"699b1772019e48b347677195e8fbb1a2473a562a27609895db050134ad222991\""
Apr 14 12:19:29 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:29.269673738Z" level=info msg="StartContainer for \"699b1772019e48b347677195e8fbb1a2473a562a27609895db050134ad222991\""
Apr 14 12:19:29 clusterfuck-worker2 systemd[1]: Started libcontainer container 699b1772019e48b347677195e8fbb1a2473a562a27609895db050134ad222991.
Apr 14 12:19:29 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:29.640881240Z" level=info msg="StartContainer for \"699b1772019e48b347677195e8fbb1a2473a562a27609895db050134ad222991\" returns successfully"
Apr 14 12:19:30 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:30.212610896Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:30 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:30.212721401Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:30 clusterfuck-worker2 containerd[101]: time="2023-04-14T12:19:30.212843785Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:30 clusterfuck-worker2 kubelet[217]: I0414 12:19:30.313211     217 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
Apr 14 12:19:53 clusterfuck-worker2 systemd[1]: Starting Cleanup of Temporary Directories...
Apr 14 12:19:53 clusterfuck-worker2 systemd[1]: systemd-tmpfiles-clean.service: Deactivated successfully.
Apr 14 12:19:53 clusterfuck-worker2 systemd[1]: Finished Cleanup of Temporary Directories.
Apr 14 12:24:23 clusterfuck-worker2 kubelet[217]: W0414 12:24:23.745307     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:29:23 clusterfuck-worker2 kubelet[217]: W0414 12:29:23.744776     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:34:23 clusterfuck-worker2 kubelet[217]: W0414 12:34:23.745576     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:39:23 clusterfuck-worker2 kubelet[217]: W0414 12:39:23.734315     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:44:23 clusterfuck-worker2 kubelet[217]: W0414 12:44:23.732352     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:49:23 clusterfuck-worker2 kubelet[217]: W0414 12:49:23.731176     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:54:23 clusterfuck-worker2 kubelet[217]: W0414 12:54:23.728980     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:59:23 clusterfuck-worker2 kubelet[217]: W0414 12:59:23.727137     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:04:23 clusterfuck-worker2 kubelet[217]: W0414 13:04:23.724271     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:09:23 clusterfuck-worker2 kubelet[217]: W0414 13:09:23.723770     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:14:23 clusterfuck-worker2 kubelet[217]: W0414 13:14:23.694288     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:19:23 clusterfuck-worker2 kubelet[217]: W0414 13:19:23.689388     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:24:23 clusterfuck-worker2 kubelet[217]: W0414 13:24:23.687381     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:29:23 clusterfuck-worker2 kubelet[217]: W0414 13:29:23.682716     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:34:23 clusterfuck-worker2 kubelet[217]: W0414 13:34:23.671540     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:39:23 clusterfuck-worker2 kubelet[217]: W0414 13:39:23.666079     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:44:23 clusterfuck-worker2 kubelet[217]: W0414 13:44:23.662066     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
