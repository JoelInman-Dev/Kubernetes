Apr 14 12:18:37 clusterfuck-worker2 systemd[1]: Condition check resulted in kubelet: The Kubernetes Node Agent being skipped.
Apr 14 12:19:20 clusterfuck-worker2 systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Apr 14 12:19:20 clusterfuck-worker2 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: I0414 12:19:20.793840     170 server.go:198] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker2 kubelet[170]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.145625     170 server.go:412] "Kubelet version" kubeletVersion="v1.26.3"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.145681     170 server.go:414] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.146392     170 server.go:836] "Client rotation is on, will bootstrap in background"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.152156     170 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153548     170 container_manager_linux.go:267] "Container manager verified user specified cgroup-root exists" cgroupRoot=[kubelet]
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153648     170 container_manager_linux.go:272] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/kubelet CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153819     170 topology_manager.go:134] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153875     170 container_manager_linux.go:308] "Creating device plugin manager"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.153995     170 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.154239     170 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.163560     170 kubelet.go:398] "Attempting to sync node with API server"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.163621     170 kubelet.go:286] "Adding static pod path" path="/etc/kubernetes/manifests"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.163646     170 kubelet.go:297] "Adding apiserver pod source"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.163657     170 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.165522     170 kuberuntime_manager.go:244] "Container runtime initialized" containerRuntime="containerd" version="v1.6.19-46-g941215f49" apiVersion="v1"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.165783     170 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.166181     170 server.go:1186] "Started kubelet"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.167770     170 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.168248     170 server.go:161] "Starting to listen" address="0.0.0.0" port=10250
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.168975     170 server.go:451] "Adding debug handlers to kubelet server"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.182040     170 volume_manager.go:293] "Starting Kubelet Volume Manager"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.182580     170 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.204968     170 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205146     170 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.205301     170 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker2" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205358     170 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker2" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: W0414 12:19:22.205419     170 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205470     170 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205578     170 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker2.1755cc6329b2e204", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker2", UID:"clusterfuck-worker2", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker2"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 166166020, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 166166020, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.205988     170 controller.go:146] failed to ensure lease exists, will retry in 200ms, error: leases.coordination.k8s.io "clusterfuck-worker2" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.254328     170 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker2.1755cc632ed2a885", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker2", UID:"clusterfuck-worker2", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker2 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker2"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252134533, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252134533, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.255818     170 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker2.1755cc632ed2be6b", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker2", UID:"clusterfuck-worker2", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker2 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker2"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252140139, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252140139, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.256516     170 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker2.1755cc632ed2c763", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker2", UID:"clusterfuck-worker2", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker2 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker2"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252142435, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 252142435, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.258020     170 cpu_manager.go:214] "Starting CPU manager" policy="none"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.258067     170 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.258086     170 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.260230     170 policy_none.go:49] "None policy: Start"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.261297     170 memory_manager.go:169] "Starting memorymanager" policy="None"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.261351     170 state_mem.go:35] "Initializing new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: I0414 12:19:22.284447     170 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker2"
Apr 14 12:19:22 clusterfuck-worker2 kubelet[170]: E0414 12:19:22.297847     170 kubelet.go:1466] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: root container [kubelet kubepods] doesn't exist"
Apr 14 12:19:22 clusterfuck-worker2 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Apr 14 12:19:22 clusterfuck-worker2 systemd[1]: kubelet.service: Failed with result 'exit-code'.
Apr 14 12:19:22 clusterfuck-worker2 systemd[1]: kubelet.service: Consumed 1.090s CPU time.
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1.
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: kubelet.service: Consumed 1.090s CPU time.
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Apr 14 12:19:23 clusterfuck-worker2 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.652468     217 server.go:198] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.662105     217 server.go:412] "Kubelet version" kubeletVersion="v1.26.3"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.662212     217 server.go:414] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.662492     217 server.go:836] "Client rotation is on, will bootstrap in background"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.663982     217 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.670853     217 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: W0414 12:19:23.671271     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676008     217 container_manager_linux.go:267] "Container manager verified user specified cgroup-root exists" cgroupRoot=[kubelet]
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676094     217 container_manager_linux.go:272] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/kubelet CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676119     217 topology_manager.go:134] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676133     217 container_manager_linux.go:308] "Creating device plugin manager"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.676174     217 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.690636     217 kubelet.go:398] "Attempting to sync node with API server"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.690690     217 kubelet.go:286] "Adding static pod path" path="/etc/kubernetes/manifests"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.690759     217 kubelet.go:297] "Adding apiserver pod source"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.690773     217 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.692472     217 kuberuntime_manager.go:244] "Container runtime initialized" containerRuntime="containerd" version="v1.6.19-46-g941215f49" apiVersion="v1"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.693509     217 server.go:1186] "Started kubelet"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.699319     217 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.704230     217 server.go:161] "Starting to listen" address="0.0.0.0" port=10250
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.705047     217 server.go:451] "Adding debug handlers to kubelet server"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.716422     217 volume_manager.go:293] "Starting Kubelet Volume Manager"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.722176     217 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.747887     217 cpu_manager.go:214] "Starting CPU manager" policy="none"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.747909     217 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.747926     217 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.748073     217 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.748087     217 state_mem.go:96] "Updated CPUSet assignments" assignments=map[]
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.748093     217 policy_none.go:49] "None policy: Start"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.749018     217 memory_manager.go:169] "Starting memorymanager" policy="None"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.749038     217 state_mem.go:35] "Initializing new in-memory state store"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.749141     217 state_mem.go:75] "Updated machine memory state"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: E0414 12:19:23.753961     217 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"clusterfuck-worker2\" not found" node="clusterfuck-worker2"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.789070     217 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.789428     217 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: E0414 12:19:23.794495     217 eviction_manager.go:261] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"clusterfuck-worker2\" not found"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.813979     217 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.825048     217 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker2"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.852018     217 kubelet_node_status.go:73] "Successfully registered node" node="clusterfuck-worker2"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.883200     217 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.883366     217 status_manager.go:176] "Starting to sync pod status with apiserver"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: I0414 12:19:23.883987     217 kubelet.go:2113] "Starting kubelet main sync loop"
Apr 14 12:19:23 clusterfuck-worker2 kubelet[217]: E0414 12:19:23.885137     217 kubelet.go:2137] "Skipping pod synchronization" err="PLEG is not healthy: pleg has yet to be successful"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.003569     217 kuberuntime_manager.go:1114] "Updating runtime config through cri with podcidr" CIDR="10.244.1.0/24"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.004559     217 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.1.0/24"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.692514     217 apiserver.go:52] "Watching apiserver"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.694520     217 topology_manager.go:210] "Topology Admit Handler"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.694722     217 topology_manager.go:210] "Topology Admit Handler"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.723376     217 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.728850     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/e7826347-ae03-4685-ba88-c51ebe1c48d9-xtables-lock\") pod \"kindnet-vxxpt\" (UID: \"e7826347-ae03-4685-ba88-c51ebe1c48d9\") " pod="kube-system/kindnet-vxxpt"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.728990     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/e7826347-ae03-4685-ba88-c51ebe1c48d9-lib-modules\") pod \"kindnet-vxxpt\" (UID: \"e7826347-ae03-4685-ba88-c51ebe1c48d9\") " pod="kube-system/kindnet-vxxpt"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729135     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zz9h4\" (UniqueName: \"kubernetes.io/projected/e7826347-ae03-4685-ba88-c51ebe1c48d9-kube-api-access-zz9h4\") pod \"kindnet-vxxpt\" (UID: \"e7826347-ae03-4685-ba88-c51ebe1c48d9\") " pod="kube-system/kindnet-vxxpt"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729241     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/ce844715-cb8a-44d7-8bfd-75f7392433d9-kube-proxy\") pod \"kube-proxy-rtmjr\" (UID: \"ce844715-cb8a-44d7-8bfd-75f7392433d9\") " pod="kube-system/kube-proxy-rtmjr"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729379     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/ce844715-cb8a-44d7-8bfd-75f7392433d9-xtables-lock\") pod \"kube-proxy-rtmjr\" (UID: \"ce844715-cb8a-44d7-8bfd-75f7392433d9\") " pod="kube-system/kube-proxy-rtmjr"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729524     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ce844715-cb8a-44d7-8bfd-75f7392433d9-lib-modules\") pod \"kube-proxy-rtmjr\" (UID: \"ce844715-cb8a-44d7-8bfd-75f7392433d9\") " pod="kube-system/kube-proxy-rtmjr"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729672     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sc456\" (UniqueName: \"kubernetes.io/projected/ce844715-cb8a-44d7-8bfd-75f7392433d9-kube-api-access-sc456\") pod \"kube-proxy-rtmjr\" (UID: \"ce844715-cb8a-44d7-8bfd-75f7392433d9\") " pod="kube-system/kube-proxy-rtmjr"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729854     217 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/e7826347-ae03-4685-ba88-c51ebe1c48d9-cni-cfg\") pod \"kindnet-vxxpt\" (UID: \"e7826347-ae03-4685-ba88-c51ebe1c48d9\") " pod="kube-system/kindnet-vxxpt"
Apr 14 12:19:24 clusterfuck-worker2 kubelet[217]: I0414 12:19:24.729974     217 reconciler.go:41] "Reconciler: start to sync state"
Apr 14 12:19:27 clusterfuck-worker2 kubelet[217]: I0414 12:19:27.928385     217 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-rtmjr" podStartSLOduration=4.928342753 pod.CreationTimestamp="2023-04-14 12:19:23 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-04-14 12:19:27.928214066 +0000 UTC m=+4.347222233" watchObservedRunningTime="2023-04-14 12:19:27.928342753 +0000 UTC m=+4.347350924"
Apr 14 12:19:30 clusterfuck-worker2 kubelet[217]: I0414 12:19:30.313211     217 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
Apr 14 12:24:23 clusterfuck-worker2 kubelet[217]: W0414 12:24:23.745307     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:29:23 clusterfuck-worker2 kubelet[217]: W0414 12:29:23.744776     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:34:23 clusterfuck-worker2 kubelet[217]: W0414 12:34:23.745576     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:39:23 clusterfuck-worker2 kubelet[217]: W0414 12:39:23.734315     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:44:23 clusterfuck-worker2 kubelet[217]: W0414 12:44:23.732352     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:49:23 clusterfuck-worker2 kubelet[217]: W0414 12:49:23.731176     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:54:23 clusterfuck-worker2 kubelet[217]: W0414 12:54:23.728980     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:59:23 clusterfuck-worker2 kubelet[217]: W0414 12:59:23.727137     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:04:23 clusterfuck-worker2 kubelet[217]: W0414 13:04:23.724271     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:09:23 clusterfuck-worker2 kubelet[217]: W0414 13:09:23.723770     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:14:23 clusterfuck-worker2 kubelet[217]: W0414 13:14:23.694288     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:19:23 clusterfuck-worker2 kubelet[217]: W0414 13:19:23.689388     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:24:23 clusterfuck-worker2 kubelet[217]: W0414 13:24:23.687381     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:29:23 clusterfuck-worker2 kubelet[217]: W0414 13:29:23.682716     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:34:23 clusterfuck-worker2 kubelet[217]: W0414 13:34:23.671540     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:39:23 clusterfuck-worker2 kubelet[217]: W0414 13:39:23.666079     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:44:23 clusterfuck-worker2 kubelet[217]: W0414 13:44:23.662066     217 sysinfo.go:203] Nodes topology is not available, providing CPU topology
