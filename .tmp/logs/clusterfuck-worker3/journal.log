Apr 14 12:18:38 clusterfuck-worker3 systemd-journald[88]: Journal started
Apr 14 12:18:38 clusterfuck-worker3 systemd-journald[88]: Runtime Journal (/run/log/journal/dc78cd23415c4954bbae137f72ba2528) is 8.0M, max 800.5M, 792.5M free.
Apr 14 12:18:38 clusterfuck-worker3 systemd-journald[88]: Runtime Journal (/run/log/journal/dc78cd23415c4954bbae137f72ba2528) is 8.0M, max 800.5M, 792.5M free.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Starting Flush Journal to Persistent Storage...
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Finished Flush Journal to Persistent Storage.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Finished Create Static Device Nodes in /dev.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Reached target Preparation for Local File Systems.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Reached target Local File Systems.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Condition check resulted in Set Up Additional Binary Formats being skipped.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Condition check resulted in Store a System Token in an EFI Variable being skipped.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Condition check resulted in Commit a transient machine-id on disk being skipped.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Condition check resulted in Rule-based Manager for Device Events and Files being skipped.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Reached target System Initialization.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Started Daily Cleanup of Temporary Directories.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Reached target Basic System.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Reached target Timer Units.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Starting containerd container runtime...
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Condition check resulted in kubelet: The Kubernetes Node Agent being skipped.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Started containerd container runtime.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Reached target Multi-User System.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Reached target Graphical Interface.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Starting Record Runlevel Change in UTMP...
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: systemd-update-utmp-runlevel.service: Deactivated successfully.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Finished Record Runlevel Change in UTMP.
Apr 14 12:18:38 clusterfuck-worker3 systemd[1]: Startup finished in 1.832s.
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.060094084Z" level=info msg="starting containerd" revision=941215f4987b5d303cedbbf9f8c9c4fa99b903ff version=v1.6.19-46-g941215f49
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.245200671Z" level=info msg="loading plugin \"io.containerd.content.v1.content\"..." type=io.containerd.content.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.245376400Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.aufs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.248261137Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.aufs\"..." error="aufs is not supported (modprobe aufs failed: exit status 1 \"modprobe: FATAL: Module aufs not found in directory /lib/modules/5.15.49-linuxkit\\n\"): skip plugin" type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.248374357Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.260763157Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.btrfs (ext4) must be a btrfs filesystem to be used with the btrfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.266060860Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.devmapper\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.266147072Z" level=warning msg="failed to load plugin io.containerd.snapshotter.v1.devmapper" error="devmapper not configured"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.266223042Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.native\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.266309655Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.overlayfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.266594209Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.zfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.266814844Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.zfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.273118107Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.fuse-overlayfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.278179853Z" level=info msg="loading plugin \"io.containerd.metadata.v1.bolt\"..." type=io.containerd.metadata.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.278597156Z" level=warning msg="could not use snapshotter devmapper in metadata plugin" error="devmapper not configured"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.278661160Z" level=info msg="metadata content store policy set" policy=shared
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.278913417Z" level=info msg="loading plugin \"io.containerd.differ.v1.walking\"..." type=io.containerd.differ.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.284555025Z" level=info msg="loading plugin \"io.containerd.event.v1.exchange\"..." type=io.containerd.event.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.284686202Z" level=info msg="loading plugin \"io.containerd.gc.v1.scheduler\"..." type=io.containerd.gc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.284777001Z" level=info msg="loading plugin \"io.containerd.service.v1.introspection-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.284834899Z" level=info msg="loading plugin \"io.containerd.service.v1.containers-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.284887315Z" level=info msg="loading plugin \"io.containerd.service.v1.content-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.287788359Z" level=info msg="loading plugin \"io.containerd.service.v1.diff-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.287942588Z" level=info msg="loading plugin \"io.containerd.service.v1.images-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.288004605Z" level=info msg="loading plugin \"io.containerd.service.v1.leases-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.298033053Z" level=info msg="loading plugin \"io.containerd.service.v1.namespaces-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.298172500Z" level=info msg="loading plugin \"io.containerd.service.v1.snapshots-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.299583144Z" level=info msg="loading plugin \"io.containerd.runtime.v1.linux\"..." type=io.containerd.runtime.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.299705825Z" level=info msg="loading plugin \"io.containerd.runtime.v2.task\"..." type=io.containerd.runtime.v2
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.299792798Z" level=info msg="loading plugin \"io.containerd.monitor.v1.cgroups\"..." type=io.containerd.monitor.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.313244306Z" level=info msg="loading plugin \"io.containerd.service.v1.tasks-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.313372291Z" level=info msg="loading plugin \"io.containerd.grpc.v1.introspection\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.322228425Z" level=info msg="loading plugin \"io.containerd.internal.v1.restart\"..." type=io.containerd.internal.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.322452464Z" level=info msg="loading plugin \"io.containerd.grpc.v1.containers\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.323123751Z" level=info msg="loading plugin \"io.containerd.grpc.v1.content\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.334995746Z" level=info msg="loading plugin \"io.containerd.grpc.v1.diff\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.335148309Z" level=info msg="loading plugin \"io.containerd.grpc.v1.events\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.335221358Z" level=info msg="loading plugin \"io.containerd.grpc.v1.healthcheck\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.335332985Z" level=info msg="loading plugin \"io.containerd.grpc.v1.images\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.335481431Z" level=info msg="loading plugin \"io.containerd.grpc.v1.leases\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.335587830Z" level=info msg="loading plugin \"io.containerd.grpc.v1.namespaces\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.335651620Z" level=info msg="loading plugin \"io.containerd.internal.v1.opt\"..." type=io.containerd.internal.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.338252102Z" level=info msg="loading plugin \"io.containerd.grpc.v1.snapshots\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.338405012Z" level=info msg="loading plugin \"io.containerd.grpc.v1.tasks\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.338577604Z" level=info msg="loading plugin \"io.containerd.grpc.v1.version\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.338692495Z" level=info msg="loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." type=io.containerd.tracing.processor.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.338752563Z" level=info msg="skip loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." error="no OpenTelemetry endpoint: skip plugin" type=io.containerd.tracing.processor.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.339001746Z" level=info msg="loading plugin \"io.containerd.internal.v1.tracing\"..." type=io.containerd.internal.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.346595917Z" level=error msg="failed to initialize a tracing processor \"otlp\"" error="no OpenTelemetry endpoint: skip plugin"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.346850323Z" level=info msg="loading plugin \"io.containerd.grpc.v1.cri\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.347531297Z" level=info msg="Start cri plugin with config {PluginConfig:{ContainerdConfig:{Snapshotter:overlayfs DefaultRuntimeName:runc DefaultRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} UntrustedWorkloadRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} Runtimes:map[runc:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[SystemdCgroup:true] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:/etc/containerd/cri-base.json NetworkPluginConfDir: NetworkPluginMaxConfNum:0} test-handler:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[SystemdCgroup:true] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:/etc/containerd/cri-base.json NetworkPluginConfDir: NetworkPluginMaxConfNum:0}] NoPivot:false DisableSnapshotAnnotations:true DiscardUnpackedLayers:true IgnoreRdtNotEnabledErrors:false} CniConfig:{NetworkPluginBinDir:/opt/cni/bin NetworkPluginConfDir:/etc/cni/net.d NetworkPluginMaxConfNum:1 NetworkPluginConfTemplate: IPPreference:} Registry:{ConfigPath: Mirrors:map[] Configs:map[] Auths:map[] Headers:map[]} ImageDecryption:{KeyModel:node} DisableTCPService:true StreamServerAddress:127.0.0.1 StreamServerPort:0 StreamIdleTimeout:4h0m0s EnableSelinux:false SelinuxCategoryRange:1024 SandboxImage:registry.k8s.io/pause:3.7 StatsCollectPeriod:10 SystemdCgroup:false EnableTLSStreaming:false X509KeyPairStreaming:{TLSCertFile: TLSKeyFile:} MaxContainerLogLineSize:16384 DisableCgroup:false DisableApparmor:false RestrictOOMScoreAdj:false MaxConcurrentDownloads:3 DisableProcMount:false UnsetSeccompProfile: TolerateMissingHugetlbController:true DisableHugetlbController:true DeviceOwnershipFromSecurityContext:false IgnoreImageDefinedVolumes:false NetNSMountsUnderStateDir:false EnableUnprivilegedPorts:false EnableUnprivilegedICMP:false} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/run/containerd/io.containerd.grpc.v1.cri}"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.351387469Z" level=info msg="Connect containerd service"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.351587879Z" level=info msg="Get image filesystem path \"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\""
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.357697209Z" level=error msg="failed to load cni during init, please check CRI plugin status before setting up network for pods" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.359480290Z" level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.359528799Z" level=info msg=serving... address=/run/containerd/containerd.sock
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.359549206Z" level=info msg="containerd successfully booted in 0.300340s"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.361346456Z" level=info msg="Start subscribing containerd event"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.361647447Z" level=info msg="Start recovering state"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.452905260Z" level=warning msg="The image docker.io/kindest/kindnetd:v20230330-48f316cd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.510345866Z" level=warning msg="The image docker.io/kindest/local-path-helper:v20230330-48f316cd@sha256:135203f2441f916fb13dad1561d27f60a6f11f50ec288b01a7d2ee9947c36270 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.541639155Z" level=warning msg="The image docker.io/kindest/local-path-provisioner:v0.0.23-kind.0@sha256:f2d0a02831ff3a03cf51343226670d5060623b43a4cfc4808bd0875b2c4b9501 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.555157175Z" level=warning msg="The image import-2023-03-30@sha256:3dd2337f70af979c7362b5e52bbdfcb3a5fd39c78d94d02145150cd2db86ba39 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.573774315Z" level=warning msg="The image import-2023-03-30@sha256:44db4d50a5f9c8efbac0d37ea974d1c0419a5928f90748d3d491a041a00c20b5 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.581054584Z" level=warning msg="The image import-2023-03-30@sha256:8dbb345de79d1c44f59a7895da702a5f71997ae72aea056609445c397b0c10dc is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.588515729Z" level=warning msg="The image import-2023-03-30@sha256:ba097b515c8c40689733c0f19de377e9bf8995964b7d7150c2045f3dfd166657 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.595930330Z" level=warning msg="The image registry.k8s.io/coredns/coredns:v1.9.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.604200650Z" level=warning msg="The image registry.k8s.io/etcd:3.5.6-0 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.606581417Z" level=warning msg="The image registry.k8s.io/kube-apiserver:v1.26.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.608276920Z" level=warning msg="The image registry.k8s.io/kube-controller-manager:v1.26.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.611425613Z" level=warning msg="The image registry.k8s.io/kube-proxy:v1.26.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.612470208Z" level=warning msg="The image registry.k8s.io/kube-scheduler:v1.26.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.614143804Z" level=warning msg="The image registry.k8s.io/pause:3.7 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.617221825Z" level=warning msg="The image sha256:221177c6082a88ea4f6240ab2450d540955ac6f4d5454f0e15751b653ebda165 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.619601434Z" level=warning msg="The image sha256:37af659db0ba1408025ceab0e9344dc354a8f38dd9b39875e48b2ad8e2fc3a51 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.621951993Z" level=warning msg="The image sha256:5185b96f0becf59032b8e3646e99f84d9655dff3ac9e2605e0dc77f9c441ae4a is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.626069752Z" level=warning msg="The image sha256:801fc1f38fa6cb63d4f64438ed884330e86e4be7504f33702e8edcd6fd2118a8 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.627539627Z" level=warning msg="The image sha256:a329ae3c2c52fe00e9c4eaf48b081cd184ee4bf9aea059e497f4965f0a8deedb is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.630405962Z" level=warning msg="The image sha256:c408b2276bb76627a6f633bf0d26052c208ebd51681c6c89866cc9647471c0bc is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.635988407Z" level=warning msg="The image sha256:cb77c367deebf699996aef822193e3630ce0fe9202b04f809214ff531718fe47 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.641155696Z" level=warning msg="The image sha256:dec886d0664924af97d276417712f4a065af16e8696ea66afc5a09799e137f8e is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.642926461Z" level=warning msg="The image sha256:eb3079d47a23af78d9b29f246a472fd77838412611e8ccb980a911b34663cc12 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.647315854Z" level=warning msg="The image sha256:fce326961ae2d51a5f726883fd59d2a8c2ccc3e45d3bb859882db58e422e59e7 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.648710371Z" level=info msg="Start event monitor"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.648824000Z" level=info msg="Start snapshots syncer"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.648953358Z" level=info msg="Start cni network conf syncer for default"
Apr 14 12:18:39 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:18:39.649132476Z" level=info msg="Start streaming server"
Apr 14 12:19:20 clusterfuck-worker3 systemd[1]: Reloading.
Apr 14 12:19:20 clusterfuck-worker3 systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Apr 14 12:19:20 clusterfuck-worker3 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Apr 14 12:19:20 clusterfuck-worker3 kubelet[167]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:20 clusterfuck-worker3 kubelet[167]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker3 kubelet[167]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker3 kubelet[167]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker3 kubelet[167]: I0414 12:19:20.692752     167 server.go:198] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Apr 14 12:19:20 clusterfuck-worker3 kubelet[167]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:20 clusterfuck-worker3 kubelet[167]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker3 kubelet[167]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker3 kubelet[167]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.217320     167 server.go:412] "Kubelet version" kubeletVersion="v1.26.3"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.217376     167 server.go:414] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.217619     167 server.go:836] "Client rotation is on, will bootstrap in background"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.222774     167 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: W0414 12:19:21.233455     167 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.260673     167 container_manager_linux.go:267] "Container manager verified user specified cgroup-root exists" cgroupRoot=[kubelet]
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.260760     167 container_manager_linux.go:272] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/kubelet CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.260781     167 topology_manager.go:134] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.260792     167 container_manager_linux.go:308] "Creating device plugin manager"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.260874     167 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.273303     167 kubelet.go:398] "Attempting to sync node with API server"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.273973     167 kubelet.go:286] "Adding static pod path" path="/etc/kubernetes/manifests"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.274128     167 kubelet.go:297] "Adding apiserver pod source"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.274183     167 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.280896     167 kuberuntime_manager.go:244] "Container runtime initialized" containerRuntime="containerd" version="v1.6.19-46-g941215f49" apiVersion="v1"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: W0414 12:19:21.281250     167 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.282139     167 server.go:1186] "Started kubelet"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.283282     167 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.287215     167 server.go:161] "Starting to listen" address="0.0.0.0" port=10250
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.302408     167 server.go:451] "Adding debug handlers to kubelet server"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.310564     167 volume_manager.go:293] "Starting Kubelet Volume Manager"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.311100     167 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.356648     167 cpu_manager.go:214] "Starting CPU manager" policy="none"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.356665     167 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.356679     167 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.368268     167 policy_none.go:49] "None policy: Start"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.371095     167 memory_manager.go:169] "Starting memorymanager" policy="None"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: I0414 12:19:21.371124     167 state_mem.go:35] "Initializing new in-memory state store"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: E0414 12:19:21.395005     167 controller.go:146] failed to ensure lease exists, will retry in 200ms, error: leases.coordination.k8s.io "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: W0414 12:19:21.395173     167 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: E0414 12:19:21.395246     167 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: W0414 12:19:21.395327     167 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: E0414 12:19:21.395382     167 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: E0414 12:19:21.395452     167 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc62f500d86c", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 21, 282082924, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 21, 282082924, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: W0414 12:19:21.395823     167 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: E0414 12:19:21.395847     167 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:21 clusterfuck-worker3 systemd[1]: Created slice libcontainer container kubelet-kubepods.slice.
Apr 14 12:19:21 clusterfuck-worker3 kubelet[167]: E0414 12:19:21.450619     167 kubelet.go:1466] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: root container [kubelet kubepods] doesn't exist"
Apr 14 12:19:21 clusterfuck-worker3 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Apr 14 12:19:21 clusterfuck-worker3 systemd[1]: kubelet.service: Failed with result 'exit-code'.
Apr 14 12:19:22 clusterfuck-worker3 systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1.
Apr 14 12:19:22 clusterfuck-worker3 systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Apr 14 12:19:22 clusterfuck-worker3 systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Apr 14 12:19:22 clusterfuck-worker3 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.702296     198 server.go:198] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.714528     198 server.go:412] "Kubelet version" kubeletVersion="v1.26.3"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.714639     198 server.go:414] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.715102     198 server.go:836] "Client rotation is on, will bootstrap in background"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.715781     198 bootstrap.go:240] unable to read existing bootstrap client config from /etc/kubernetes/kubelet.conf: invalid configuration: [unable to read client-cert /var/lib/kubelet/pki/kubelet-client-current.pem for default-auth due to open /var/lib/kubelet/pki/kubelet-client-current.pem: no such file or directory, unable to read client-key /var/lib/kubelet/pki/kubelet-client-current.pem for default-auth due to open /var/lib/kubelet/pki/kubelet-client-current.pem: no such file or directory]
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.719084     198 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: W0414 12:19:22.723024     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.735237     198 container_manager_linux.go:267] "Container manager verified user specified cgroup-root exists" cgroupRoot=[kubelet]
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.735394     198 container_manager_linux.go:272] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/kubelet CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.735498     198 topology_manager.go:134] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.735555     198 container_manager_linux.go:308] "Creating device plugin manager"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.735620     198 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.741496     198 kubelet.go:398] "Attempting to sync node with API server"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.741605     198 kubelet.go:286] "Adding static pod path" path="/etc/kubernetes/manifests"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.741672     198 kubelet.go:297] "Adding apiserver pod source"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.741773     198 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.745185     198 kuberuntime_manager.go:244] "Container runtime initialized" containerRuntime="containerd" version="v1.6.19-46-g941215f49" apiVersion="v1"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.748180     198 server.go:1186] "Started kubelet"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.749808     198 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.764971     198 server.go:161] "Starting to listen" address="0.0.0.0" port=10250
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.765746     198 server.go:451] "Adding debug handlers to kubelet server"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.766893     198 volume_manager.go:293] "Starting Kubelet Volume Manager"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.766904     198 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.825306     198 cpu_manager.go:214] "Starting CPU manager" policy="none"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.825355     198 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.825372     198 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.825606     198 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.825652     198 state_mem.go:96] "Updated CPUSet assignments" assignments=map[]
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.825664     198 policy_none.go:49] "None policy: Start"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.827907     198 memory_manager.go:169] "Starting memorymanager" policy="None"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.827933     198 state_mem.go:35] "Initializing new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.828235     198 state_mem.go:75] "Updated machine memory state"
Apr 14 12:19:22 clusterfuck-worker3 systemd[1]: Created slice libcontainer container kubelet-kubepods-burstable.slice.
Apr 14 12:19:22 clusterfuck-worker3 systemd[1]: Created slice libcontainer container kubelet-kubepods-besteffort.slice.
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: W0414 12:19:22.875128     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.875228     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.875394     198 controller.go:146] failed to ensure lease exists, will retry in 200ms, error: leases.coordination.k8s.io "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: W0414 12:19:22.875663     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.875683     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.876510     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc634c637114", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 748162324, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 748162324, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: W0414 12:19:22.880709     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.884035     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.884417     198 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.884699     198 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: I0414 12:19:22.890013     198 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker3"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.894518     198 eviction_manager.go:261] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"clusterfuck-worker3\" not found"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.897170     198 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker3"
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.897169     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d449ee", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822666734, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822666734, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.901227     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d46dd4", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker3 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822675924, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822675924, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.915851     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d4780d", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822678541, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822678541, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.917994     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d449ee", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822666734, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 889980998, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d449ee" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.926597     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d46dd4", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker3 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822675924, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 889985382, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d46dd4" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.963669     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d4780d", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822678541, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 889987274, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d4780d" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker3 kubelet[198]: E0414 12:19:22.969907     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6355224573", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeAllocatableEnforced", Message:"Updated Node Allocatable limit across pods", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 894886259, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 894886259, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: I0414 12:19:23.015369     198 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: I0414 12:19:23.073513     198 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: I0414 12:19:23.074090     198 status_manager.go:176] "Starting to sync pod status with apiserver"
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: I0414 12:19:23.074217     198 kubelet.go:2113] "Starting kubelet main sync loop"
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.074436     198 kubelet.go:2137] "Skipping pod synchronization" err="PLEG is not healthy: pleg has yet to be successful"
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.080646     198 controller.go:146] failed to ensure lease exists, will retry in 400ms, error: leases.coordination.k8s.io "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: W0414 12:19:23.080908     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.080989     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: I0414 12:19:23.099103     198 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker3"
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.104813     198 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker3"
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.105752     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d449ee", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822666734, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 99074514, time.Local), Count:3, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d449ee" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.108761     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d46dd4", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker3 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822675924, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 99078261, time.Local), Count:3, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d46dd4" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.111393     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d4780d", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822678541, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 99080350, time.Local), Count:3, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d4780d" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.482661     198 controller.go:146] failed to ensure lease exists, will retry in 800ms, error: leases.coordination.k8s.io "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: I0414 12:19:23.506373     198 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker3"
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.509157     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d449ee", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822666734, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 506344779, time.Local), Count:4, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d449ee" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.510464     198 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker3"
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.511416     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d46dd4", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker3 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822675924, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 506348929, time.Local), Count:4, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d46dd4" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker3 kubelet[198]: E0414 12:19:23.553051     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d4780d", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822678541, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 506350819, time.Local), Count:4, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d4780d" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: W0414 12:19:24.185947     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: E0414 12:19:24.185989     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: W0414 12:19:24.209253     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: E0414 12:19:24.209312     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: E0414 12:19:24.284960     198 controller.go:146] failed to ensure lease exists, will retry in 1.6s, error: leases.coordination.k8s.io "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: W0414 12:19:24.311620     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: E0414 12:19:24.311754     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: I0414 12:19:24.311728     198 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker3"
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: E0414 12:19:24.313041     198 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker3"
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: E0414 12:19:24.314187     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d449ee", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822666734, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 24, 311695922, time.Local), Count:5, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d449ee" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: E0414 12:19:24.317254     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d46dd4", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker3 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822675924, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 24, 311706194, time.Local), Count:5, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d46dd4" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: E0414 12:19:24.318760     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d4780d", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822678541, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 24, 311708210, time.Local), Count:5, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d4780d" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: W0414 12:19:24.436589     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker3 kubelet[198]: E0414 12:19:24.436648     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:25 clusterfuck-worker3 kubelet[198]: E0414 12:19:25.888105     198 controller.go:146] failed to ensure lease exists, will retry in 3.2s, error: leases.coordination.k8s.io "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:25 clusterfuck-worker3 kubelet[198]: I0414 12:19:25.914593     198 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker3"
Apr 14 12:19:25 clusterfuck-worker3 kubelet[198]: E0414 12:19:25.916223     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d449ee", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822666734, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 25, 914560094, time.Local), Count:6, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d449ee" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:25 clusterfuck-worker3 kubelet[198]: E0414 12:19:25.916980     198 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker3"
Apr 14 12:19:25 clusterfuck-worker3 kubelet[198]: E0414 12:19:25.917229     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d46dd4", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker3 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822675924, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 25, 914563666, time.Local), Count:6, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d46dd4" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:25 clusterfuck-worker3 kubelet[198]: E0414 12:19:25.920495     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d4780d", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822678541, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 25, 914569718, time.Local), Count:6, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d4780d" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:26 clusterfuck-worker3 kubelet[198]: W0414 12:19:26.221265     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:26 clusterfuck-worker3 kubelet[198]: E0414 12:19:26.221326     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:26 clusterfuck-worker3 kubelet[198]: W0414 12:19:26.222317     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:26 clusterfuck-worker3 kubelet[198]: E0414 12:19:26.222371     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:26 clusterfuck-worker3 kubelet[198]: W0414 12:19:26.458880     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:26 clusterfuck-worker3 kubelet[198]: E0414 12:19:26.459009     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:27 clusterfuck-worker3 kubelet[198]: W0414 12:19:27.171521     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:27 clusterfuck-worker3 kubelet[198]: E0414 12:19:27.171586     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:29 clusterfuck-worker3 kubelet[198]: E0414 12:19:29.090541     198 controller.go:146] failed to ensure lease exists, will retry in 6.4s, error: leases.coordination.k8s.io "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:29 clusterfuck-worker3 kubelet[198]: I0414 12:19:29.118656     198 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker3"
Apr 14 12:19:29 clusterfuck-worker3 kubelet[198]: E0414 12:19:29.120270     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d449ee", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822666734, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 29, 118610289, time.Local), Count:7, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d449ee" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:29 clusterfuck-worker3 kubelet[198]: E0414 12:19:29.120573     198 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker3"
Apr 14 12:19:29 clusterfuck-worker3 kubelet[198]: E0414 12:19:29.121096     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d46dd4", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker3 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822675924, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 29, 118614378, time.Local), Count:7, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d46dd4" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:29 clusterfuck-worker3 kubelet[198]: E0414 12:19:29.123791     198 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker3.1755cc6350d4780d", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker3", UID:"clusterfuck-worker3", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker3 status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker3"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 822678541, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 29, 118632854, time.Local), Count:7, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker3.1755cc6350d4780d" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:30 clusterfuck-worker3 kubelet[198]: W0414 12:19:30.093466     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:30 clusterfuck-worker3 kubelet[198]: E0414 12:19:30.093496     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:30 clusterfuck-worker3 kubelet[198]: W0414 12:19:30.693528     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:30 clusterfuck-worker3 kubelet[198]: E0414 12:19:30.693574     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:31 clusterfuck-worker3 kubelet[198]: W0414 12:19:31.785342     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:31 clusterfuck-worker3 kubelet[198]: E0414 12:19:31.785837     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:32 clusterfuck-worker3 kubelet[198]: W0414 12:19:32.093131     198 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:32 clusterfuck-worker3 kubelet[198]: E0414 12:19:32.093197     198 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker3" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:32 clusterfuck-worker3 kubelet[198]: I0414 12:19:32.734483     198 transport.go:135] "Certificate rotation detected, shutting down client connections to start using new credentials"
Apr 14 12:19:32 clusterfuck-worker3 kubelet[198]: E0414 12:19:32.894850     198 eviction_manager.go:261] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"clusterfuck-worker3\" not found"
Apr 14 12:19:33 clusterfuck-worker3 kubelet[198]: E0414 12:19:33.221551     198 csi_plugin.go:295] Failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes "clusterfuck-worker3" not found
Apr 14 12:19:34 clusterfuck-worker3 kubelet[198]: E0414 12:19:34.281187     198 csi_plugin.go:295] Failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes "clusterfuck-worker3" not found
Apr 14 12:19:35 clusterfuck-worker3 kubelet[198]: E0414 12:19:35.495592     198 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"clusterfuck-worker3\" not found" node="clusterfuck-worker3"
Apr 14 12:19:35 clusterfuck-worker3 kubelet[198]: I0414 12:19:35.521871     198 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker3"
Apr 14 12:19:35 clusterfuck-worker3 kubelet[198]: I0414 12:19:35.684245     198 kubelet_node_status.go:73] "Successfully registered node" node="clusterfuck-worker3"
Apr 14 12:19:35 clusterfuck-worker3 kubelet[198]: E0414 12:19:35.691691     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:35 clusterfuck-worker3 kubelet[198]: E0414 12:19:35.793147     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:35 clusterfuck-worker3 kubelet[198]: E0414 12:19:35.893817     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:35 clusterfuck-worker3 kubelet[198]: E0414 12:19:35.994243     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:36 clusterfuck-worker3 kubelet[198]: E0414 12:19:36.094924     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:36 clusterfuck-worker3 kubelet[198]: E0414 12:19:36.196212     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:36 clusterfuck-worker3 kubelet[198]: E0414 12:19:36.296716     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:36 clusterfuck-worker3 kubelet[198]: E0414 12:19:36.398006     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:36 clusterfuck-worker3 kubelet[198]: E0414 12:19:36.499120     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:36 clusterfuck-worker3 kubelet[198]: E0414 12:19:36.599938     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:36 clusterfuck-worker3 kubelet[198]: E0414 12:19:36.700888     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:36 clusterfuck-worker3 kubelet[198]: E0414 12:19:36.801808     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:36 clusterfuck-worker3 kubelet[198]: E0414 12:19:36.902068     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.003123     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.103355     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.204060     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.305350     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.406337     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.506714     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.607825     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.708319     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.809302     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:37 clusterfuck-worker3 kubelet[198]: E0414 12:19:37.911110     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.011989     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.112596     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.213756     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.314588     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.415773     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.516475     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.616960     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.718265     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.818742     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:38 clusterfuck-worker3 kubelet[198]: E0414 12:19:38.919441     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.020597     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.121754     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.222789     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.323838     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.424794     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.525755     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.626749     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.728045     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.829263     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:39 clusterfuck-worker3 kubelet[198]: E0414 12:19:39.929512     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.030555     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.131877     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.233378     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.334803     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.435336     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.535516     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.636067     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.736632     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.836980     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:40 clusterfuck-worker3 kubelet[198]: E0414 12:19:40.938110     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.039275     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.140363     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.242774     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.343566     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.444089     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.545111     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.645942     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.746143     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.847183     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:41 clusterfuck-worker3 kubelet[198]: E0414 12:19:41.948238     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.048756     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.152357     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.254964     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.355541     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.456465     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.557415     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.658187     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.759401     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.860441     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.896221     198 eviction_manager.go:261] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"clusterfuck-worker3\" not found"
Apr 14 12:19:42 clusterfuck-worker3 kubelet[198]: E0414 12:19:42.961376     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.061599     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.162786     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.263843     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.364840     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.464980     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.565670     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.665902     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.766822     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.867755     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:43 clusterfuck-worker3 kubelet[198]: E0414 12:19:43.968581     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: E0414 12:19:44.068895     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: E0414 12:19:44.169222     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: E0414 12:19:44.269365     198 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"clusterfuck-worker3\" not found"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.371522     198 kuberuntime_manager.go:1114] "Updating runtime config through cri with podcidr" CIDR="10.244.2.0/24"
Apr 14 12:19:44 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:44.371955775Z" level=info msg="No cni config template is specified, wait for other system components to drop the config."
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.372490     198 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.2.0/24"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.763206     198 apiserver.go:52] "Watching apiserver"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.765286     198 topology_manager.go:210] "Topology Admit Handler"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.765383     198 topology_manager.go:210] "Topology Admit Handler"
Apr 14 12:19:44 clusterfuck-worker3 systemd[1]: Starting Cleanup of Temporary Directories...
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.779145     198 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Apr 14 12:19:44 clusterfuck-worker3 systemd[1]: systemd-tmpfiles-clean.service: Deactivated successfully.
Apr 14 12:19:44 clusterfuck-worker3 systemd[1]: Finished Cleanup of Temporary Directories.
Apr 14 12:19:44 clusterfuck-worker3 systemd[1]: Created slice libcontainer container kubelet-kubepods-besteffort-podf10c5303_69d9_4894_ba0f_18a022d5c192.slice.
Apr 14 12:19:44 clusterfuck-worker3 systemd[1]: Created slice libcontainer container kubelet-kubepods-podaf9fa247_e423_4011_8374_4130976b71df.slice.
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.866288     198 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/f10c5303-69d9-4894-ba0f-18a022d5c192-kube-proxy\") pod \"kube-proxy-cvv2r\" (UID: \"f10c5303-69d9-4894-ba0f-18a022d5c192\") " pod="kube-system/kube-proxy-cvv2r"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.866362     198 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/f10c5303-69d9-4894-ba0f-18a022d5c192-xtables-lock\") pod \"kube-proxy-cvv2r\" (UID: \"f10c5303-69d9-4894-ba0f-18a022d5c192\") " pod="kube-system/kube-proxy-cvv2r"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.866394     198 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/f10c5303-69d9-4894-ba0f-18a022d5c192-lib-modules\") pod \"kube-proxy-cvv2r\" (UID: \"f10c5303-69d9-4894-ba0f-18a022d5c192\") " pod="kube-system/kube-proxy-cvv2r"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.866413     198 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4jzr5\" (UniqueName: \"kubernetes.io/projected/f10c5303-69d9-4894-ba0f-18a022d5c192-kube-api-access-4jzr5\") pod \"kube-proxy-cvv2r\" (UID: \"f10c5303-69d9-4894-ba0f-18a022d5c192\") " pod="kube-system/kube-proxy-cvv2r"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.866431     198 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/af9fa247-e423-4011-8374-4130976b71df-cni-cfg\") pod \"kindnet-grscj\" (UID: \"af9fa247-e423-4011-8374-4130976b71df\") " pod="kube-system/kindnet-grscj"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.866449     198 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/af9fa247-e423-4011-8374-4130976b71df-xtables-lock\") pod \"kindnet-grscj\" (UID: \"af9fa247-e423-4011-8374-4130976b71df\") " pod="kube-system/kindnet-grscj"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.866466     198 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/af9fa247-e423-4011-8374-4130976b71df-lib-modules\") pod \"kindnet-grscj\" (UID: \"af9fa247-e423-4011-8374-4130976b71df\") " pod="kube-system/kindnet-grscj"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.866550     198 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-crg5x\" (UniqueName: \"kubernetes.io/projected/af9fa247-e423-4011-8374-4130976b71df-kube-api-access-crg5x\") pod \"kindnet-grscj\" (UID: \"af9fa247-e423-4011-8374-4130976b71df\") " pod="kube-system/kindnet-grscj"
Apr 14 12:19:44 clusterfuck-worker3 kubelet[198]: I0414 12:19:44.866562     198 reconciler.go:41] "Reconciler: start to sync state"
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.102756092Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-cvv2r,Uid:f10c5303-69d9-4894-ba0f-18a022d5c192,Namespace:kube-system,Attempt:0,}"
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.116326310Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-grscj,Uid:af9fa247-e423-4011-8374-4130976b71df,Namespace:kube-system,Attempt:0,}"
Apr 14 12:19:45 clusterfuck-worker3 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount1924789811.mount: Deactivated successfully.
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.153893261Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.154020918Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.154060731Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.154204163Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/702dfaad65178e498799a13dc11b7b98c7cf47706d68f9d6a8f3784c5841257d pid=280 runtime=io.containerd.runc.v2
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.166023385Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.166107852Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.166120005Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.166561979Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/7a95fabc4195a102ba73374ef0f599385e9ed5deadcbae01fd468dcb61bfb205 pid=301 runtime=io.containerd.runc.v2
Apr 14 12:19:45 clusterfuck-worker3 systemd[1]: Started libcontainer container 702dfaad65178e498799a13dc11b7b98c7cf47706d68f9d6a8f3784c5841257d.
Apr 14 12:19:45 clusterfuck-worker3 systemd[1]: Started libcontainer container 7a95fabc4195a102ba73374ef0f599385e9ed5deadcbae01fd468dcb61bfb205.
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.233751138Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-cvv2r,Uid:f10c5303-69d9-4894-ba0f-18a022d5c192,Namespace:kube-system,Attempt:0,} returns sandbox id \"702dfaad65178e498799a13dc11b7b98c7cf47706d68f9d6a8f3784c5841257d\""
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.239736554Z" level=info msg="CreateContainer within sandbox \"702dfaad65178e498799a13dc11b7b98c7cf47706d68f9d6a8f3784c5841257d\" for container &ContainerMetadata{Name:kube-proxy,Attempt:0,}"
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.537871361Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-grscj,Uid:af9fa247-e423-4011-8374-4130976b71df,Namespace:kube-system,Attempt:0,} returns sandbox id \"7a95fabc4195a102ba73374ef0f599385e9ed5deadcbae01fd468dcb61bfb205\""
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.541421622Z" level=info msg="PullImage \"docker.io/kindest/kindnetd:v20230330-48f316cd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af\""
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.908980529Z" level=info msg="CreateContainer within sandbox \"702dfaad65178e498799a13dc11b7b98c7cf47706d68f9d6a8f3784c5841257d\" for &ContainerMetadata{Name:kube-proxy,Attempt:0,} returns container id \"6fb7deffc7e3c68eb8b2feb51bc5f04fd64dde12c5a64785f434f27f66743fdc\""
Apr 14 12:19:45 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:45.909916748Z" level=info msg="StartContainer for \"6fb7deffc7e3c68eb8b2feb51bc5f04fd64dde12c5a64785f434f27f66743fdc\""
Apr 14 12:19:45 clusterfuck-worker3 systemd[1]: Started libcontainer container 6fb7deffc7e3c68eb8b2feb51bc5f04fd64dde12c5a64785f434f27f66743fdc.
Apr 14 12:19:45 clusterfuck-worker3 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount2980765080.mount: Deactivated successfully.
Apr 14 12:19:46 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:46.046383698Z" level=info msg="StartContainer for \"6fb7deffc7e3c68eb8b2feb51bc5f04fd64dde12c5a64785f434f27f66743fdc\" returns successfully"
Apr 14 12:19:46 clusterfuck-worker3 kubelet[198]: I0414 12:19:46.144765     198 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-cvv2r" podStartSLOduration=11.144510267 pod.CreationTimestamp="2023-04-14 12:19:35 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-04-14 12:19:46.144098327 +0000 UTC m=+23.552187821" watchObservedRunningTime="2023-04-14 12:19:46.144510267 +0000 UTC m=+23.552599769"
Apr 14 12:19:47 clusterfuck-worker3 systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount727653030.mount: Deactivated successfully.
Apr 14 12:19:47 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:47.781311771Z" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/kindest/kindnetd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 12:19:47 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:47.785165655Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:a329ae3c2c52fe00e9c4eaf48b081cd184ee4bf9aea059e497f4965f0a8deedb,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 12:19:47 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:47.788615495Z" level=info msg="PullImage \"docker.io/kindest/kindnetd:v20230330-48f316cd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af\" returns image reference \"sha256:a329ae3c2c52fe00e9c4eaf48b081cd184ee4bf9aea059e497f4965f0a8deedb\""
Apr 14 12:19:47 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:47.791688916Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/kindest/kindnetd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 12:19:47 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:47.793600075Z" level=info msg="CreateContainer within sandbox \"7a95fabc4195a102ba73374ef0f599385e9ed5deadcbae01fd468dcb61bfb205\" for container &ContainerMetadata{Name:kindnet-cni,Attempt:0,}"
Apr 14 12:19:47 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:47.805627175Z" level=info msg="CreateContainer within sandbox \"7a95fabc4195a102ba73374ef0f599385e9ed5deadcbae01fd468dcb61bfb205\" for &ContainerMetadata{Name:kindnet-cni,Attempt:0,} returns container id \"f0c694b084ae2a10002030ce4d53b468d22a99334dac0eb376e78ebbdbf49a1c\""
Apr 14 12:19:47 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:47.807339628Z" level=info msg="StartContainer for \"f0c694b084ae2a10002030ce4d53b468d22a99334dac0eb376e78ebbdbf49a1c\""
Apr 14 12:19:47 clusterfuck-worker3 systemd[1]: Started libcontainer container f0c694b084ae2a10002030ce4d53b468d22a99334dac0eb376e78ebbdbf49a1c.
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.227729589Z" level=info msg="StartContainer for \"f0c694b084ae2a10002030ce4d53b468d22a99334dac0eb376e78ebbdbf49a1c\" returns successfully"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.911959499Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.912065866Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.912220718Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.912273938Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.912422969Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.912478106Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.912800350Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.912920637Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.913115070Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.913294887Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.913385744Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.913487487Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.913689828Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.913856206Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:48 clusterfuck-worker3 containerd[101]: time="2023-04-14T12:19:48.914024506Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:49 clusterfuck-worker3 kubelet[198]: I0414 12:19:49.016361     198 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
Apr 14 12:24:22 clusterfuck-worker3 kubelet[198]: W0414 12:24:22.822454     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:29:22 clusterfuck-worker3 kubelet[198]: W0414 12:29:22.821410     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:34:22 clusterfuck-worker3 kubelet[198]: W0414 12:34:22.820293     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:39:22 clusterfuck-worker3 kubelet[198]: W0414 12:39:22.812248     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:44:22 clusterfuck-worker3 kubelet[198]: W0414 12:44:22.810736     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:49:22 clusterfuck-worker3 kubelet[198]: W0414 12:49:22.808148     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:54:22 clusterfuck-worker3 kubelet[198]: W0414 12:54:22.806214     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:59:22 clusterfuck-worker3 kubelet[198]: W0414 12:59:22.805166     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:04:22 clusterfuck-worker3 kubelet[198]: W0414 13:04:22.803873     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:09:22 clusterfuck-worker3 kubelet[198]: W0414 13:09:22.800979     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:14:22 clusterfuck-worker3 kubelet[198]: W0414 13:14:22.773609     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:19:22 clusterfuck-worker3 kubelet[198]: W0414 13:19:22.767215     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:24:22 clusterfuck-worker3 kubelet[198]: W0414 13:24:22.765068     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:29:22 clusterfuck-worker3 kubelet[198]: W0414 13:29:22.759663     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:34:22 clusterfuck-worker3 kubelet[198]: W0414 13:34:22.752367     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:39:22 clusterfuck-worker3 kubelet[198]: W0414 13:39:22.744644     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:44:22 clusterfuck-worker3 kubelet[198]: W0414 13:44:22.740486     198 sysinfo.go:203] Nodes topology is not available, providing CPU topology
