Apr 14 12:18:38 clusterfuck-worker systemd-journald[88]: Journal started
Apr 14 12:18:38 clusterfuck-worker systemd-journald[88]: Runtime Journal (/run/log/journal/af0b4d75110a40408db6bf1f664c4e81) is 8.0M, max 800.5M, 792.5M free.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Starting Flush Journal to Persistent Storage...
Apr 14 12:18:38 clusterfuck-worker systemd-journald[88]: Runtime Journal (/run/log/journal/af0b4d75110a40408db6bf1f664c4e81) is 8.0M, max 800.5M, 792.5M free.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Finished Flush Journal to Persistent Storage.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Finished Create Static Device Nodes in /dev.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Reached target Preparation for Local File Systems.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Reached target Local File Systems.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Condition check resulted in Set Up Additional Binary Formats being skipped.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Condition check resulted in Store a System Token in an EFI Variable being skipped.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Condition check resulted in Commit a transient machine-id on disk being skipped.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Condition check resulted in Rule-based Manager for Device Events and Files being skipped.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Reached target System Initialization.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Started Daily Cleanup of Temporary Directories.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Reached target Basic System.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Reached target Timer Units.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Starting containerd container runtime...
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Condition check resulted in kubelet: The Kubernetes Node Agent being skipped.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Started containerd container runtime.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Reached target Multi-User System.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Reached target Graphical Interface.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Starting Record Runlevel Change in UTMP...
Apr 14 12:18:38 clusterfuck-worker systemd[1]: systemd-update-utmp-runlevel.service: Deactivated successfully.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Finished Record Runlevel Change in UTMP.
Apr 14 12:18:38 clusterfuck-worker systemd[1]: Startup finished in 2.020s.
Apr 14 12:18:38 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:38.955333943Z" level=info msg="starting containerd" revision=941215f4987b5d303cedbbf9f8c9c4fa99b903ff version=v1.6.19-46-g941215f49
Apr 14 12:18:38 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:38.999557699Z" level=info msg="loading plugin \"io.containerd.content.v1.content\"..." type=io.containerd.content.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.012501927Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.aufs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.057652725Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.aufs\"..." error="aufs is not supported (modprobe aufs failed: exit status 1 \"modprobe: FATAL: Module aufs not found in directory /lib/modules/5.15.49-linuxkit\\n\"): skip plugin" type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.057767207Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.057962372Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.btrfs (ext4) must be a btrfs filesystem to be used with the btrfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.058035842Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.devmapper\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.058086933Z" level=warning msg="failed to load plugin io.containerd.snapshotter.v1.devmapper" error="devmapper not configured"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.058132654Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.native\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.058193643Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.overlayfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.059098858Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.zfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.059335502Z" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.zfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.059409724Z" level=info msg="loading plugin \"io.containerd.snapshotter.v1.fuse-overlayfs\"..." type=io.containerd.snapshotter.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.065037345Z" level=info msg="loading plugin \"io.containerd.metadata.v1.bolt\"..." type=io.containerd.metadata.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.065194439Z" level=warning msg="could not use snapshotter devmapper in metadata plugin" error="devmapper not configured"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.065260520Z" level=info msg="metadata content store policy set" policy=shared
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.065801508Z" level=info msg="loading plugin \"io.containerd.differ.v1.walking\"..." type=io.containerd.differ.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.071722781Z" level=info msg="loading plugin \"io.containerd.event.v1.exchange\"..." type=io.containerd.event.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.071813746Z" level=info msg="loading plugin \"io.containerd.gc.v1.scheduler\"..." type=io.containerd.gc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.071890592Z" level=info msg="loading plugin \"io.containerd.service.v1.introspection-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.071951028Z" level=info msg="loading plugin \"io.containerd.service.v1.containers-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072002102Z" level=info msg="loading plugin \"io.containerd.service.v1.content-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072058150Z" level=info msg="loading plugin \"io.containerd.service.v1.diff-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072108332Z" level=info msg="loading plugin \"io.containerd.service.v1.images-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072157135Z" level=info msg="loading plugin \"io.containerd.service.v1.leases-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072203607Z" level=info msg="loading plugin \"io.containerd.service.v1.namespaces-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072254584Z" level=info msg="loading plugin \"io.containerd.service.v1.snapshots-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072302969Z" level=info msg="loading plugin \"io.containerd.runtime.v1.linux\"..." type=io.containerd.runtime.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072388938Z" level=info msg="loading plugin \"io.containerd.runtime.v2.task\"..." type=io.containerd.runtime.v2
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072478364Z" level=info msg="loading plugin \"io.containerd.monitor.v1.cgroups\"..." type=io.containerd.monitor.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072710704Z" level=info msg="loading plugin \"io.containerd.service.v1.tasks-service\"..." type=io.containerd.service.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072776867Z" level=info msg="loading plugin \"io.containerd.grpc.v1.introspection\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072831516Z" level=info msg="loading plugin \"io.containerd.internal.v1.restart\"..." type=io.containerd.internal.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072915099Z" level=info msg="loading plugin \"io.containerd.grpc.v1.containers\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.072966957Z" level=info msg="loading plugin \"io.containerd.grpc.v1.content\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073021378Z" level=info msg="loading plugin \"io.containerd.grpc.v1.diff\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073069206Z" level=info msg="loading plugin \"io.containerd.grpc.v1.events\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073115566Z" level=info msg="loading plugin \"io.containerd.grpc.v1.healthcheck\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073166712Z" level=info msg="loading plugin \"io.containerd.grpc.v1.images\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073215286Z" level=info msg="loading plugin \"io.containerd.grpc.v1.leases\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073267834Z" level=info msg="loading plugin \"io.containerd.grpc.v1.namespaces\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073319959Z" level=info msg="loading plugin \"io.containerd.internal.v1.opt\"..." type=io.containerd.internal.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073420239Z" level=info msg="loading plugin \"io.containerd.grpc.v1.snapshots\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073472232Z" level=info msg="loading plugin \"io.containerd.grpc.v1.tasks\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073520854Z" level=info msg="loading plugin \"io.containerd.grpc.v1.version\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073573164Z" level=info msg="loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." type=io.containerd.tracing.processor.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073627562Z" level=info msg="skip loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." error="no OpenTelemetry endpoint: skip plugin" type=io.containerd.tracing.processor.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073674072Z" level=info msg="loading plugin \"io.containerd.internal.v1.tracing\"..." type=io.containerd.internal.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073739090Z" level=error msg="failed to initialize a tracing processor \"otlp\"" error="no OpenTelemetry endpoint: skip plugin"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.073811024Z" level=info msg="loading plugin \"io.containerd.grpc.v1.cri\"..." type=io.containerd.grpc.v1
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.074034186Z" level=info msg="Start cri plugin with config {PluginConfig:{ContainerdConfig:{Snapshotter:overlayfs DefaultRuntimeName:runc DefaultRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} UntrustedWorkloadRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} Runtimes:map[runc:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[SystemdCgroup:true] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:/etc/containerd/cri-base.json NetworkPluginConfDir: NetworkPluginMaxConfNum:0} test-handler:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[SystemdCgroup:true] PrivilegedWithoutHostDevices:false BaseRuntimeSpec:/etc/containerd/cri-base.json NetworkPluginConfDir: NetworkPluginMaxConfNum:0}] NoPivot:false DisableSnapshotAnnotations:true DiscardUnpackedLayers:true IgnoreRdtNotEnabledErrors:false} CniConfig:{NetworkPluginBinDir:/opt/cni/bin NetworkPluginConfDir:/etc/cni/net.d NetworkPluginMaxConfNum:1 NetworkPluginConfTemplate: IPPreference:} Registry:{ConfigPath: Mirrors:map[] Configs:map[] Auths:map[] Headers:map[]} ImageDecryption:{KeyModel:node} DisableTCPService:true StreamServerAddress:127.0.0.1 StreamServerPort:0 StreamIdleTimeout:4h0m0s EnableSelinux:false SelinuxCategoryRange:1024 SandboxImage:registry.k8s.io/pause:3.7 StatsCollectPeriod:10 SystemdCgroup:false EnableTLSStreaming:false X509KeyPairStreaming:{TLSCertFile: TLSKeyFile:} MaxContainerLogLineSize:16384 DisableCgroup:false DisableApparmor:false RestrictOOMScoreAdj:false MaxConcurrentDownloads:3 DisableProcMount:false UnsetSeccompProfile: TolerateMissingHugetlbController:true DisableHugetlbController:true DeviceOwnershipFromSecurityContext:false IgnoreImageDefinedVolumes:false NetNSMountsUnderStateDir:false EnableUnprivilegedPorts:false EnableUnprivilegedICMP:false} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/run/containerd/io.containerd.grpc.v1.cri}"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.074265808Z" level=info msg="Connect containerd service"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.075190572Z" level=info msg="Get image filesystem path \"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\""
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.075688950Z" level=error msg="failed to load cni during init, please check CRI plugin status before setting up network for pods" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.099888968Z" level=info msg="Start subscribing containerd event"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.104138007Z" level=info msg="Start recovering state"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.118615241Z" level=warning msg="The image docker.io/kindest/kindnetd:v20230330-48f316cd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.128766058Z" level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.128972134Z" level=info msg=serving... address=/run/containerd/containerd.sock
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.129099252Z" level=info msg="containerd successfully booted in 0.174655s"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.144140155Z" level=warning msg="The image docker.io/kindest/local-path-helper:v20230330-48f316cd@sha256:135203f2441f916fb13dad1561d27f60a6f11f50ec288b01a7d2ee9947c36270 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.159245792Z" level=warning msg="The image docker.io/kindest/local-path-provisioner:v0.0.23-kind.0@sha256:f2d0a02831ff3a03cf51343226670d5060623b43a4cfc4808bd0875b2c4b9501 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.163635182Z" level=warning msg="The image import-2023-03-30@sha256:3dd2337f70af979c7362b5e52bbdfcb3a5fd39c78d94d02145150cd2db86ba39 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.165387988Z" level=warning msg="The image import-2023-03-30@sha256:44db4d50a5f9c8efbac0d37ea974d1c0419a5928f90748d3d491a041a00c20b5 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.178324351Z" level=warning msg="The image import-2023-03-30@sha256:8dbb345de79d1c44f59a7895da702a5f71997ae72aea056609445c397b0c10dc is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.184279845Z" level=warning msg="The image import-2023-03-30@sha256:ba097b515c8c40689733c0f19de377e9bf8995964b7d7150c2045f3dfd166657 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.185649260Z" level=warning msg="The image registry.k8s.io/coredns/coredns:v1.9.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.208480932Z" level=warning msg="The image registry.k8s.io/etcd:3.5.6-0 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.257662640Z" level=warning msg="The image registry.k8s.io/kube-apiserver:v1.26.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.273511505Z" level=warning msg="The image registry.k8s.io/kube-controller-manager:v1.26.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.285627226Z" level=warning msg="The image registry.k8s.io/kube-proxy:v1.26.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.309375313Z" level=warning msg="The image registry.k8s.io/kube-scheduler:v1.26.3 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.310221981Z" level=warning msg="The image registry.k8s.io/pause:3.7 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.321571331Z" level=warning msg="The image sha256:221177c6082a88ea4f6240ab2450d540955ac6f4d5454f0e15751b653ebda165 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.331204626Z" level=warning msg="The image sha256:37af659db0ba1408025ceab0e9344dc354a8f38dd9b39875e48b2ad8e2fc3a51 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.347295795Z" level=warning msg="The image sha256:5185b96f0becf59032b8e3646e99f84d9655dff3ac9e2605e0dc77f9c441ae4a is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.355798394Z" level=warning msg="The image sha256:801fc1f38fa6cb63d4f64438ed884330e86e4be7504f33702e8edcd6fd2118a8 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.372504373Z" level=warning msg="The image sha256:a329ae3c2c52fe00e9c4eaf48b081cd184ee4bf9aea059e497f4965f0a8deedb is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.389442057Z" level=warning msg="The image sha256:c408b2276bb76627a6f633bf0d26052c208ebd51681c6c89866cc9647471c0bc is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.410231513Z" level=warning msg="The image sha256:cb77c367deebf699996aef822193e3630ce0fe9202b04f809214ff531718fe47 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.423345075Z" level=warning msg="The image sha256:dec886d0664924af97d276417712f4a065af16e8696ea66afc5a09799e137f8e is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.423812104Z" level=warning msg="The image sha256:eb3079d47a23af78d9b29f246a472fd77838412611e8ccb980a911b34663cc12 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.424537680Z" level=warning msg="The image sha256:fce326961ae2d51a5f726883fd59d2a8c2ccc3e45d3bb859882db58e422e59e7 is not unpacked."
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.425358588Z" level=info msg="Start event monitor"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.425413100Z" level=info msg="Start snapshots syncer"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.425474163Z" level=info msg="Start cni network conf syncer for default"
Apr 14 12:18:39 clusterfuck-worker containerd[101]: time="2023-04-14T12:18:39.425483284Z" level=info msg="Start streaming server"
Apr 14 12:19:20 clusterfuck-worker systemd[1]: Reloading.
Apr 14 12:19:20 clusterfuck-worker systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Apr 14 12:19:20 clusterfuck-worker systemd[1]: Started kubelet: The Kubernetes Node Agent.
Apr 14 12:19:20 clusterfuck-worker kubelet[166]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:20 clusterfuck-worker kubelet[166]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker kubelet[166]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker kubelet[166]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker kubelet[166]: I0414 12:19:20.794856     166 server.go:198] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Apr 14 12:19:20 clusterfuck-worker kubelet[166]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:20 clusterfuck-worker kubelet[166]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker kubelet[166]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:20 clusterfuck-worker kubelet[166]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.187679     166 server.go:412] "Kubelet version" kubeletVersion="v1.26.3"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.187829     166 server.go:414] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.188083     166 server.go:836] "Client rotation is on, will bootstrap in background"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: W0414 12:19:21.195537     166 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.197192     166 container_manager_linux.go:267] "Container manager verified user specified cgroup-root exists" cgroupRoot=[kubelet]
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.200381     166 container_manager_linux.go:272] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/kubelet CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.199120     166 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.200985     166 topology_manager.go:134] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.201276     166 container_manager_linux.go:308] "Creating device plugin manager"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.201480     166 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.249263     166 kubelet.go:398] "Attempting to sync node with API server"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.249438     166 kubelet.go:286] "Adding static pod path" path="/etc/kubernetes/manifests"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.249570     166 kubelet.go:297] "Adding apiserver pod source"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.249666     166 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.251899     166 kuberuntime_manager.go:244] "Container runtime initialized" containerRuntime="containerd" version="v1.6.19-46-g941215f49" apiVersion="v1"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: W0414 12:19:21.252372     166 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.257889     166 server.go:1186] "Started kubelet"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.266453     166 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.268334     166 server.go:161] "Starting to listen" address="0.0.0.0" port=10250
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.269674     166 server.go:451] "Adding debug handlers to kubelet server"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.280912     166 volume_manager.go:293] "Starting Kubelet Volume Manager"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.283117     166 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.369385     166 cpu_manager.go:214] "Starting CPU manager" policy="none"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.369495     166 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.369549     166 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.371134     166 policy_none.go:49] "None policy: Start"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.372437     166 memory_manager.go:169] "Starting memorymanager" policy="None"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.372580     166 state_mem.go:35] "Initializing new in-memory state store"
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: I0414 12:19:21.391377     166 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker"
Apr 14 12:19:21 clusterfuck-worker systemd[1]: Created slice libcontainer container kubelet-kubepods.slice.
Apr 14 12:19:21 clusterfuck-worker kubelet[166]: E0414 12:19:21.422552     166 kubelet.go:1466] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: root container [kubelet kubepods] doesn't exist"
Apr 14 12:19:21 clusterfuck-worker systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Apr 14 12:19:21 clusterfuck-worker systemd[1]: kubelet.service: Failed with result 'exit-code'.
Apr 14 12:19:22 clusterfuck-worker systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 1.
Apr 14 12:19:22 clusterfuck-worker systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Apr 14 12:19:22 clusterfuck-worker systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Apr 14 12:19:22 clusterfuck-worker systemd[1]: Started kubelet: The Kubernetes Node Agent.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.721165     195 server.go:198] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: Flag --provider-id has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.728358     195 server.go:412] "Kubelet version" kubeletVersion="v1.26.3"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.728380     195 server.go:414] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.728608     195 server.go:836] "Client rotation is on, will bootstrap in background"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.729183     195 bootstrap.go:240] unable to read existing bootstrap client config from /etc/kubernetes/kubelet.conf: invalid configuration: [unable to read client-cert /var/lib/kubelet/pki/kubelet-client-current.pem for default-auth due to open /var/lib/kubelet/pki/kubelet-client-current.pem: no such file or directory, unable to read client-key /var/lib/kubelet/pki/kubelet-client-current.pem for default-auth due to open /var/lib/kubelet/pki/kubelet-client-current.pem: no such file or directory]
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: W0414 12:19:22.732509     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.734473     195 container_manager_linux.go:267] "Container manager verified user specified cgroup-root exists" cgroupRoot=[kubelet]
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.734519     195 container_manager_linux.go:272] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/kubelet CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.734539     195 topology_manager.go:134] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.734550     195 container_manager_linux.go:308] "Creating device plugin manager"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.734583     195 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.738027     195 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.750559     195 kubelet.go:398] "Attempting to sync node with API server"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.750620     195 kubelet.go:286] "Adding static pod path" path="/etc/kubernetes/manifests"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.750642     195 kubelet.go:297] "Adding apiserver pod source"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.750654     195 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.766426     195 kuberuntime_manager.go:244] "Container runtime initialized" containerRuntime="containerd" version="v1.6.19-46-g941215f49" apiVersion="v1"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.767924     195 server.go:1186] "Started kubelet"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.769364     195 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.773424     195 server.go:161] "Starting to listen" address="0.0.0.0" port=10250
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.774239     195 server.go:451] "Adding debug handlers to kubelet server"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.778531     195 volume_manager.go:293] "Starting Kubelet Volume Manager"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.779705     195 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.852917     195 cpu_manager.go:214] "Starting CPU manager" policy="none"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.853065     195 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.853089     195 state_mem.go:36] "Initialized new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.853322     195 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.853368     195 state_mem.go:96] "Updated CPUSet assignments" assignments=map[]
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.853377     195 policy_none.go:49] "None policy: Start"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.856855     195 controller.go:146] failed to ensure lease exists, will retry in 200ms, error: leases.coordination.k8s.io "clusterfuck-worker" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.857011     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc634d908795", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 767894421, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 767894421, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: W0414 12:19:22.857282     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.857335     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: W0414 12:19:22.857377     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.857386     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: W0414 12:19:22.857411     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.857424     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.862879     195 memory_manager.go:169] "Starting memorymanager" policy="None"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.862967     195 state_mem.go:35] "Initializing new in-memory state store"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.863096     195 state_mem.go:75] "Updated machine memory state"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.873320     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bc3ef", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850419695, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850419695, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.881421     195 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker"
Apr 14 12:19:22 clusterfuck-worker systemd[1]: Created slice libcontainer container kubelet-kubepods-burstable.slice.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.900201     195 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.900280     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bd6cf", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850424527, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850424527, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker systemd[1]: Created slice libcontainer container kubelet-kubepods-besteffort.slice.
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.917841     195 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.918009     195 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.918101     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bdef5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850426613, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850426613, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.925917     195 eviction_manager.go:261] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"clusterfuck-worker\" not found"
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.927814     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bc3ef", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850419695, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 881388318, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bc3ef" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.964048     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bd6cf", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850424527, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 881393550, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bd6cf" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.971054     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bdef5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850426613, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 881395789, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bdef5" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: I0414 12:19:22.975640     195 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
Apr 14 12:19:22 clusterfuck-worker kubelet[195]: E0414 12:19:22.979638     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc6356a0b299", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeAllocatableEnforced", Message:"Updated Node Allocatable limit across pods", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 919948953, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 919948953, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events is forbidden: User "system:anonymous" cannot create resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.068296     195 controller.go:146] failed to ensure lease exists, will retry in 400ms, error: leases.coordination.k8s.io "clusterfuck-worker" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: I0414 12:19:23.102172     195 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker"
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.106598     195 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker"
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.106827     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bc3ef", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850419695, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 102131091, time.Local), Count:3, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bc3ef" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.107939     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bd6cf", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850424527, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 102135734, time.Local), Count:3, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bd6cf" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.109122     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bdef5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850426613, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 102137842, time.Local), Count:3, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bdef5" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: I0414 12:19:23.131377     195 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: I0414 12:19:23.131402     195 status_manager.go:176] "Starting to sync pod status with apiserver"
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: I0414 12:19:23.131624     195 kubelet.go:2113] "Starting kubelet main sync loop"
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.131676     195 kubelet.go:2137] "Skipping pod synchronization" err="PLEG is not healthy: pleg has yet to be successful"
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: W0414 12:19:23.134210     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.134298     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.472281     195 controller.go:146] failed to ensure lease exists, will retry in 800ms, error: leases.coordination.k8s.io "clusterfuck-worker" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: I0414 12:19:23.508115     195 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker"
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.512231     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bc3ef", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850419695, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 508068187, time.Local), Count:4, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bc3ef" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.513226     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bd6cf", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850424527, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 508072484, time.Local), Count:4, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bd6cf" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.514119     195 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker"
Apr 14 12:19:23 clusterfuck-worker kubelet[195]: E0414 12:19:23.570213     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bdef5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850426613, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 23, 508093983, time.Local), Count:4, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bdef5" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: W0414 12:19:24.140790     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: E0414 12:19:24.141198     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: W0414 12:19:24.238574     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: E0414 12:19:24.238641     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: W0414 12:19:24.249098     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: E0414 12:19:24.249228     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: E0414 12:19:24.274466     195 controller.go:146] failed to ensure lease exists, will retry in 1.6s, error: leases.coordination.k8s.io "clusterfuck-worker" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: W0414 12:19:24.289480     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: E0414 12:19:24.289623     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: I0414 12:19:24.315555     195 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker"
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: E0414 12:19:24.317125     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bc3ef", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850419695, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 24, 315518246, time.Local), Count:5, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bc3ef" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: E0414 12:19:24.317410     195 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker"
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: E0414 12:19:24.318552     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bd6cf", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850424527, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 24, 315523824, time.Local), Count:5, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bd6cf" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:24 clusterfuck-worker kubelet[195]: E0414 12:19:24.320085     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bdef5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850426613, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 24, 315525859, time.Local), Count:5, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bdef5" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:25 clusterfuck-worker kubelet[195]: E0414 12:19:25.876128     195 controller.go:146] failed to ensure lease exists, will retry in 3.2s, error: leases.coordination.k8s.io "clusterfuck-worker" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:25 clusterfuck-worker kubelet[195]: I0414 12:19:25.918494     195 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker"
Apr 14 12:19:25 clusterfuck-worker kubelet[195]: E0414 12:19:25.920990     195 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker"
Apr 14 12:19:25 clusterfuck-worker kubelet[195]: E0414 12:19:25.921206     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bc3ef", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850419695, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 25, 918463172, time.Local), Count:6, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bc3ef" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:25 clusterfuck-worker kubelet[195]: E0414 12:19:25.922404     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bd6cf", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850424527, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 25, 918466663, time.Local), Count:6, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bd6cf" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:25 clusterfuck-worker kubelet[195]: E0414 12:19:25.923461     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bdef5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850426613, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 25, 918468485, time.Local), Count:6, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bdef5" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:26 clusterfuck-worker kubelet[195]: W0414 12:19:26.200411     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:26 clusterfuck-worker kubelet[195]: E0414 12:19:26.200575     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:26 clusterfuck-worker kubelet[195]: W0414 12:19:26.687395     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:26 clusterfuck-worker kubelet[195]: E0414 12:19:26.687516     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:27 clusterfuck-worker kubelet[195]: W0414 12:19:27.143665     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:27 clusterfuck-worker kubelet[195]: E0414 12:19:27.144087     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:anonymous" cannot list resource "services" in API group "" at the cluster scope
Apr 14 12:19:27 clusterfuck-worker kubelet[195]: W0414 12:19:27.309196     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes "clusterfuck-worker" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:27 clusterfuck-worker kubelet[195]: E0414 12:19:27.309330     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes "clusterfuck-worker" is forbidden: User "system:anonymous" cannot list resource "nodes" in API group "" at the cluster scope
Apr 14 12:19:29 clusterfuck-worker kubelet[195]: E0414 12:19:29.078568     195 controller.go:146] failed to ensure lease exists, will retry in 6.4s, error: leases.coordination.k8s.io "clusterfuck-worker" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-node-lease"
Apr 14 12:19:29 clusterfuck-worker kubelet[195]: I0414 12:19:29.123339     195 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker"
Apr 14 12:19:29 clusterfuck-worker kubelet[195]: E0414 12:19:29.124881     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bc3ef", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node clusterfuck-worker status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850419695, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 29, 123277590, time.Local), Count:7, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bc3ef" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:29 clusterfuck-worker kubelet[195]: E0414 12:19:29.125117     195 kubelet_node_status.go:92] "Unable to register node with API server" err="nodes is forbidden: User \"system:anonymous\" cannot create resource \"nodes\" in API group \"\" at the cluster scope" node="clusterfuck-worker"
Apr 14 12:19:29 clusterfuck-worker kubelet[195]: E0414 12:19:29.125903     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bd6cf", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node clusterfuck-worker status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850424527, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 29, 123315851, time.Local), Count:7, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bd6cf" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:29 clusterfuck-worker kubelet[195]: E0414 12:19:29.126961     195 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"clusterfuck-worker.1755cc63527bdef5", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"clusterfuck-worker", UID:"clusterfuck-worker", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node clusterfuck-worker status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"clusterfuck-worker"}, FirstTimestamp:time.Date(2023, time.April, 14, 12, 19, 22, 850426613, time.Local), LastTimestamp:time.Date(2023, time.April, 14, 12, 19, 29, 123318366, time.Local), Count:7, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "clusterfuck-worker.1755cc63527bdef5" is forbidden: User "system:anonymous" cannot patch resource "events" in API group "" in the namespace "default"' (will not retry!)
Apr 14 12:19:30 clusterfuck-worker kubelet[195]: W0414 12:19:30.108464     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:30 clusterfuck-worker kubelet[195]: E0414 12:19:30.108524     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: runtimeclasses.node.k8s.io is forbidden: User "system:anonymous" cannot list resource "runtimeclasses" in API group "node.k8s.io" at the cluster scope
Apr 14 12:19:31 clusterfuck-worker kubelet[195]: W0414 12:19:31.585923     195 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:31 clusterfuck-worker kubelet[195]: E0414 12:19:31.586026     195 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:anonymous" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
Apr 14 12:19:32 clusterfuck-worker kubelet[195]: I0414 12:19:32.738160     195 transport.go:135] "Certificate rotation detected, shutting down client connections to start using new credentials"
Apr 14 12:19:32 clusterfuck-worker kubelet[195]: E0414 12:19:32.926628     195 eviction_manager.go:261] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"clusterfuck-worker\" not found"
Apr 14 12:19:33 clusterfuck-worker kubelet[195]: E0414 12:19:33.212462     195 csi_plugin.go:295] Failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes "clusterfuck-worker" not found
Apr 14 12:19:34 clusterfuck-worker kubelet[195]: E0414 12:19:34.660448     195 csi_plugin.go:295] Failed to initialize CSINode: error updating CSINode annotation: timed out waiting for the condition; caused by: nodes "clusterfuck-worker" not found
Apr 14 12:19:34 clusterfuck-worker kubelet[195]: I0414 12:19:34.757037     195 apiserver.go:52] "Watching apiserver"
Apr 14 12:19:34 clusterfuck-worker kubelet[195]: I0414 12:19:34.880339     195 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Apr 14 12:19:34 clusterfuck-worker kubelet[195]: I0414 12:19:34.976020     195 reconciler.go:41] "Reconciler: start to sync state"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: E0414 12:19:35.485113     195 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"clusterfuck-worker\" not found" node="clusterfuck-worker"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.526602     195 kubelet_node_status.go:70] "Attempting to register node" node="clusterfuck-worker"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.867641     195 kubelet_node_status.go:73] "Successfully registered node" node="clusterfuck-worker"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.919191     195 topology_manager.go:210] "Topology Admit Handler"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.919418     195 topology_manager.go:210] "Topology Admit Handler"
Apr 14 12:19:35 clusterfuck-worker systemd[1]: Created slice libcontainer container kubelet-kubepods-pod5d95b121_091f_4390_a8b2_080477dccebe.slice.
Apr 14 12:19:35 clusterfuck-worker systemd[1]: Created slice libcontainer container kubelet-kubepods-besteffort-podf47fe42a_4ef2_46ea_9522_149345356f8c.slice.
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.987539     195 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/f47fe42a-4ef2-46ea-9522-149345356f8c-kube-proxy\") pod \"kube-proxy-5rl5l\" (UID: \"f47fe42a-4ef2-46ea-9522-149345356f8c\") " pod="kube-system/kube-proxy-5rl5l"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.987610     195 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/f47fe42a-4ef2-46ea-9522-149345356f8c-xtables-lock\") pod \"kube-proxy-5rl5l\" (UID: \"f47fe42a-4ef2-46ea-9522-149345356f8c\") " pod="kube-system/kube-proxy-5rl5l"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.987637     195 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/f47fe42a-4ef2-46ea-9522-149345356f8c-lib-modules\") pod \"kube-proxy-5rl5l\" (UID: \"f47fe42a-4ef2-46ea-9522-149345356f8c\") " pod="kube-system/kube-proxy-5rl5l"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.987656     195 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fclfm\" (UniqueName: \"kubernetes.io/projected/f47fe42a-4ef2-46ea-9522-149345356f8c-kube-api-access-fclfm\") pod \"kube-proxy-5rl5l\" (UID: \"f47fe42a-4ef2-46ea-9522-149345356f8c\") " pod="kube-system/kube-proxy-5rl5l"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.987675     195 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/5d95b121-091f-4390-a8b2-080477dccebe-cni-cfg\") pod \"kindnet-hzbqg\" (UID: \"5d95b121-091f-4390-a8b2-080477dccebe\") " pod="kube-system/kindnet-hzbqg"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.987747     195 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/5d95b121-091f-4390-a8b2-080477dccebe-xtables-lock\") pod \"kindnet-hzbqg\" (UID: \"5d95b121-091f-4390-a8b2-080477dccebe\") " pod="kube-system/kindnet-hzbqg"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.987772     195 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/5d95b121-091f-4390-a8b2-080477dccebe-lib-modules\") pod \"kindnet-hzbqg\" (UID: \"5d95b121-091f-4390-a8b2-080477dccebe\") " pod="kube-system/kindnet-hzbqg"
Apr 14 12:19:35 clusterfuck-worker kubelet[195]: I0414 12:19:35.987788     195 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v62km\" (UniqueName: \"kubernetes.io/projected/5d95b121-091f-4390-a8b2-080477dccebe-kube-api-access-v62km\") pod \"kindnet-hzbqg\" (UID: \"5d95b121-091f-4390-a8b2-080477dccebe\") " pod="kube-system/kindnet-hzbqg"
Apr 14 12:19:36 clusterfuck-worker kubelet[195]: I0414 12:19:36.007538     195 kuberuntime_manager.go:1114] "Updating runtime config through cri with podcidr" CIDR="10.244.3.0/24"
Apr 14 12:19:36 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:36.008051110Z" level=info msg="No cni config template is specified, wait for other system components to drop the config."
Apr 14 12:19:36 clusterfuck-worker kubelet[195]: I0414 12:19:36.008250     195 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.3.0/24"
Apr 14 12:19:37 clusterfuck-worker systemd[1]: Starting Cleanup of Temporary Directories...
Apr 14 12:19:37 clusterfuck-worker systemd[1]: systemd-tmpfiles-clean.service: Deactivated successfully.
Apr 14 12:19:37 clusterfuck-worker systemd[1]: Finished Cleanup of Temporary Directories.
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.139955186Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-5rl5l,Uid:f47fe42a-4ef2-46ea-9522-149345356f8c,Namespace:kube-system,Attempt:0,}"
Apr 14 12:19:37 clusterfuck-worker systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount521650071.mount: Deactivated successfully.
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.183635768Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.183780498Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.183791669Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.184073151Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/bd0cc0699a1e825ff08be712bf1704fdb314862936725b8bf91dd46a36af2132 pid=274 runtime=io.containerd.runc.v2
Apr 14 12:19:37 clusterfuck-worker systemd[1]: Started libcontainer container bd0cc0699a1e825ff08be712bf1704fdb314862936725b8bf91dd46a36af2132.
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.256912050Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kube-proxy-5rl5l,Uid:f47fe42a-4ef2-46ea-9522-149345356f8c,Namespace:kube-system,Attempt:0,} returns sandbox id \"bd0cc0699a1e825ff08be712bf1704fdb314862936725b8bf91dd46a36af2132\""
Apr 14 12:19:37 clusterfuck-worker kubelet[195]: I0414 12:19:37.258191     195 request.go:690] Waited for 1.169854489s due to client-side throttling, not priority and fairness, request: POST:https://clusterfuck-control-plane:6443/api/v1/namespaces/kube-system/serviceaccounts/kindnet/token
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.262744051Z" level=info msg="CreateContainer within sandbox \"bd0cc0699a1e825ff08be712bf1704fdb314862936725b8bf91dd46a36af2132\" for container &ContainerMetadata{Name:kube-proxy,Attempt:0,}"
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.444408112Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-hzbqg,Uid:5d95b121-091f-4390-a8b2-080477dccebe,Namespace:kube-system,Attempt:0,}"
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.488623568Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.488855752Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.488871624Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.489273206Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/8d13529c2f0ad4b72c899934e3ee7c0bbf162ba1b11b8d79baa6f0b7e8657308 pid=317 runtime=io.containerd.runc.v2
Apr 14 12:19:37 clusterfuck-worker systemd[1]: Started libcontainer container 8d13529c2f0ad4b72c899934e3ee7c0bbf162ba1b11b8d79baa6f0b7e8657308.
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.829708148Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kindnet-hzbqg,Uid:5d95b121-091f-4390-a8b2-080477dccebe,Namespace:kube-system,Attempt:0,} returns sandbox id \"8d13529c2f0ad4b72c899934e3ee7c0bbf162ba1b11b8d79baa6f0b7e8657308\""
Apr 14 12:19:37 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:37.834865711Z" level=info msg="PullImage \"docker.io/kindest/kindnetd:v20230330-48f316cd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af\""
Apr 14 12:19:38 clusterfuck-worker systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount515566566.mount: Deactivated successfully.
Apr 14 12:19:38 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:38.122462186Z" level=info msg="CreateContainer within sandbox \"bd0cc0699a1e825ff08be712bf1704fdb314862936725b8bf91dd46a36af2132\" for &ContainerMetadata{Name:kube-proxy,Attempt:0,} returns container id \"99028068c516eb243a138d2361e32c66ea0259a34c913fe79068a8866d625c90\""
Apr 14 12:19:38 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:38.124598355Z" level=info msg="StartContainer for \"99028068c516eb243a138d2361e32c66ea0259a34c913fe79068a8866d625c90\""
Apr 14 12:19:38 clusterfuck-worker systemd[1]: Started libcontainer container 99028068c516eb243a138d2361e32c66ea0259a34c913fe79068a8866d625c90.
Apr 14 12:19:38 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:38.235938438Z" level=info msg="StartContainer for \"99028068c516eb243a138d2361e32c66ea0259a34c913fe79068a8866d625c90\" returns successfully"
Apr 14 12:19:39 clusterfuck-worker systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount2228146273.mount: Deactivated successfully.
Apr 14 12:19:40 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:40.102679843Z" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/kindest/kindnetd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 12:19:40 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:40.105302796Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:a329ae3c2c52fe00e9c4eaf48b081cd184ee4bf9aea059e497f4965f0a8deedb,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 12:19:40 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:40.108072565Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/kindest/kindnetd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 12:19:40 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:40.108370679Z" level=info msg="PullImage \"docker.io/kindest/kindnetd:v20230330-48f316cd@sha256:c19d6362a6a928139820761475a38c24c0cf84d507b9ddf414a078cf627497af\" returns image reference \"sha256:a329ae3c2c52fe00e9c4eaf48b081cd184ee4bf9aea059e497f4965f0a8deedb\""
Apr 14 12:19:40 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:40.111195867Z" level=info msg="CreateContainer within sandbox \"8d13529c2f0ad4b72c899934e3ee7c0bbf162ba1b11b8d79baa6f0b7e8657308\" for container &ContainerMetadata{Name:kindnet-cni,Attempt:0,}"
Apr 14 12:19:40 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:40.125364277Z" level=info msg="CreateContainer within sandbox \"8d13529c2f0ad4b72c899934e3ee7c0bbf162ba1b11b8d79baa6f0b7e8657308\" for &ContainerMetadata{Name:kindnet-cni,Attempt:0,} returns container id \"c21dff5e7d17d04f00f4db1fad34b95248cbe0842a7f13d22c34a686845c6c9e\""
Apr 14 12:19:40 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:40.126501672Z" level=info msg="StartContainer for \"c21dff5e7d17d04f00f4db1fad34b95248cbe0842a7f13d22c34a686845c6c9e\""
Apr 14 12:19:40 clusterfuck-worker systemd[1]: Started libcontainer container c21dff5e7d17d04f00f4db1fad34b95248cbe0842a7f13d22c34a686845c6c9e.
Apr 14 12:19:40 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:40.530457518Z" level=info msg="StartContainer for \"c21dff5e7d17d04f00f4db1fad34b95248cbe0842a7f13d22c34a686845c6c9e\" returns successfully"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.115017160Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.115140524Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.115193332Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.115284011Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.115330326Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.115794906Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.115906505Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.116039845Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.116131934Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.116389046Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.116493908Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.116598042Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.116904450Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.116981865Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker containerd[101]: time="2023-04-14T12:19:41.117057639Z" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/10-kindnet.conflist.temp\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
Apr 14 12:19:41 clusterfuck-worker kubelet[195]: I0414 12:19:41.192160     195 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-5rl5l" podStartSLOduration=6.19211382 pod.CreationTimestamp="2023-04-14 12:19:35 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-04-14 12:19:39.197344898 +0000 UTC m=+16.601819893" watchObservedRunningTime="2023-04-14 12:19:41.19211382 +0000 UTC m=+18.596588813"
Apr 14 12:19:41 clusterfuck-worker kubelet[195]: I0414 12:19:41.296078     195 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
Apr 14 12:24:22 clusterfuck-worker kubelet[195]: W0414 12:24:22.837539     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:29:22 clusterfuck-worker kubelet[195]: W0414 12:29:22.836150     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:34:22 clusterfuck-worker kubelet[195]: W0414 12:34:22.835502     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:39:22 clusterfuck-worker kubelet[195]: W0414 12:39:22.827434     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:44:22 clusterfuck-worker kubelet[195]: W0414 12:44:22.826068     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:49:22 clusterfuck-worker kubelet[195]: W0414 12:49:22.823071     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:54:22 clusterfuck-worker kubelet[195]: W0414 12:54:22.821944     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 12:59:22 clusterfuck-worker kubelet[195]: W0414 12:59:22.819147     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:04:22 clusterfuck-worker kubelet[195]: W0414 13:04:22.818262     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:09:22 clusterfuck-worker kubelet[195]: W0414 13:09:22.815999     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:13:55 clusterfuck-worker kubelet[195]: I0414 13:13:55.130880     195 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kindnet-hzbqg" podStartSLOduration=-9.22336877672393e+09 pod.CreationTimestamp="2023-04-14 12:19:35 +0000 UTC" firstStartedPulling="2023-04-14 12:19:37.832069753 +0000 UTC m=+15.236544745" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-04-14 12:19:41.193675223 +0000 UTC m=+18.598150213" watchObservedRunningTime="2023-04-14 13:13:55.130846286 +0000 UTC m=+3272.585677252"
Apr 14 13:13:55 clusterfuck-worker kubelet[195]: I0414 13:13:55.131409     195 topology_manager.go:210] "Topology Admit Handler"
Apr 14 13:13:55 clusterfuck-worker systemd[1]: Created slice libcontainer container kubelet-kubepods-besteffort-pod606ce435_627e_4e76_b8db_0ba5f5fb96bf.slice.
Apr 14 13:13:55 clusterfuck-worker kubelet[195]: I0414 13:13:55.228489     195 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-p4qtv\" (UniqueName: \"kubernetes.io/projected/606ce435-627e-4e76-b8db-0ba5f5fb96bf-kube-api-access-p4qtv\") pod \"kubernetes-bootcamp-5485cc6795-gmn5x\" (UID: \"606ce435-627e-4e76-b8db-0ba5f5fb96bf\") " pod="default/kubernetes-bootcamp-5485cc6795-gmn5x"
Apr 14 13:13:55 clusterfuck-worker containerd[101]: time="2023-04-14T13:13:55.446755592Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kubernetes-bootcamp-5485cc6795-gmn5x,Uid:606ce435-627e-4e76-b8db-0ba5f5fb96bf,Namespace:default,Attempt:0,}"
Apr 14 13:13:55 clusterfuck-worker containerd[101]: time="2023-04-14T13:13:55.491507326Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Apr 14 13:13:55 clusterfuck-worker containerd[101]: time="2023-04-14T13:13:55.491563896Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Apr 14 13:13:55 clusterfuck-worker containerd[101]: time="2023-04-14T13:13:55.491573840Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Apr 14 13:13:55 clusterfuck-worker containerd[101]: time="2023-04-14T13:13:55.492219077Z" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/9753b939e2e8542c010c29be1f4a12a3e028acab3cc3f86592f011d19ce8ad41 pid=1156 runtime=io.containerd.runc.v2
Apr 14 13:13:55 clusterfuck-worker systemd[1]: Started libcontainer container 9753b939e2e8542c010c29be1f4a12a3e028acab3cc3f86592f011d19ce8ad41.
Apr 14 13:13:55 clusterfuck-worker containerd[101]: time="2023-04-14T13:13:55.579650827Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:kubernetes-bootcamp-5485cc6795-gmn5x,Uid:606ce435-627e-4e76-b8db-0ba5f5fb96bf,Namespace:default,Attempt:0,} returns sandbox id \"9753b939e2e8542c010c29be1f4a12a3e028acab3cc3f86592f011d19ce8ad41\""
Apr 14 13:13:55 clusterfuck-worker containerd[101]: time="2023-04-14T13:13:55.582659358Z" level=info msg="PullImage \"gcr.io/google-samples/kubernetes-bootcamp:v1\""
Apr 14 13:14:08 clusterfuck-worker systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount2637814994.mount: Deactivated successfully.
Apr 14 13:14:11 clusterfuck-worker containerd[101]: time="2023-04-14T13:14:11.132176622Z" level=info msg="ImageCreate event &ImageCreate{Name:gcr.io/google-samples/kubernetes-bootcamp:v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 13:14:11 clusterfuck-worker containerd[101]: time="2023-04-14T13:14:11.133991617Z" level=info msg="ImageCreate event &ImageCreate{Name:sha256:8fafd8af70e9aa7c3ab40222ca4fd58050cf3e49cb14a4e7c0f460cd4f78e9fe,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 13:14:11 clusterfuck-worker containerd[101]: time="2023-04-14T13:14:11.135304854Z" level=info msg="ImageUpdate event &ImageUpdate{Name:gcr.io/google-samples/kubernetes-bootcamp:v1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 13:14:11 clusterfuck-worker containerd[101]: time="2023-04-14T13:14:11.136875932Z" level=info msg="ImageCreate event &ImageCreate{Name:gcr.io/google-samples/kubernetes-bootcamp@sha256:0d6b8ee63bb57c5f5b6156f446b3bc3b3c143d233037f3a2f00e279c8fcc64af,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Apr 14 13:14:11 clusterfuck-worker containerd[101]: time="2023-04-14T13:14:11.137770743Z" level=info msg="PullImage \"gcr.io/google-samples/kubernetes-bootcamp:v1\" returns image reference \"sha256:8fafd8af70e9aa7c3ab40222ca4fd58050cf3e49cb14a4e7c0f460cd4f78e9fe\""
Apr 14 13:14:11 clusterfuck-worker containerd[101]: time="2023-04-14T13:14:11.143404716Z" level=info msg="CreateContainer within sandbox \"9753b939e2e8542c010c29be1f4a12a3e028acab3cc3f86592f011d19ce8ad41\" for container &ContainerMetadata{Name:kubernetes-bootcamp,Attempt:0,}"
Apr 14 13:14:11 clusterfuck-worker systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount1793654931.mount: Deactivated successfully.
Apr 14 13:14:11 clusterfuck-worker systemd[1]: var-lib-containerd-tmpmounts-containerd\x2dmount1290312740.mount: Deactivated successfully.
Apr 14 13:14:11 clusterfuck-worker containerd[101]: time="2023-04-14T13:14:11.161823181Z" level=info msg="CreateContainer within sandbox \"9753b939e2e8542c010c29be1f4a12a3e028acab3cc3f86592f011d19ce8ad41\" for &ContainerMetadata{Name:kubernetes-bootcamp,Attempt:0,} returns container id \"94a6543882dc8b4fa56340ac44d8cd1c7f941c97ed1e37b3b593a41772e18279\""
Apr 14 13:14:11 clusterfuck-worker containerd[101]: time="2023-04-14T13:14:11.163015735Z" level=info msg="StartContainer for \"94a6543882dc8b4fa56340ac44d8cd1c7f941c97ed1e37b3b593a41772e18279\""
Apr 14 13:14:11 clusterfuck-worker systemd[1]: Started libcontainer container 94a6543882dc8b4fa56340ac44d8cd1c7f941c97ed1e37b3b593a41772e18279.
Apr 14 13:14:11 clusterfuck-worker containerd[101]: time="2023-04-14T13:14:11.293965023Z" level=info msg="StartContainer for \"94a6543882dc8b4fa56340ac44d8cd1c7f941c97ed1e37b3b593a41772e18279\" returns successfully"
Apr 14 13:14:22 clusterfuck-worker kubelet[195]: W0414 13:14:22.786237     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:19:22 clusterfuck-worker kubelet[195]: W0414 13:19:22.784049     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:24:22 clusterfuck-worker kubelet[195]: W0414 13:24:22.779062     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:29:22 clusterfuck-worker kubelet[195]: W0414 13:29:22.774810     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:34:22 clusterfuck-worker kubelet[195]: W0414 13:34:22.765027     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:39:22 clusterfuck-worker kubelet[195]: W0414 13:39:22.758497     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 14 13:44:22 clusterfuck-worker kubelet[195]: W0414 13:44:22.754650     195 sysinfo.go:203] Nodes topology is not available, providing CPU topology
